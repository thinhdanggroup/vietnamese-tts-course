{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ch04-title",
   "metadata": {},
   "source": [
    "# Chapter 4 — Neural Audio Codecs: Turning Sound into Tokens\n",
    "\n",
    "**Goal:** Understand how continuous audio waveforms are compressed into discrete integer tokens that an LLM can model.\n",
    "\n",
    "**What you will build:**\n",
    "- A Vector Quantizer from scratch (2D, visualized)\n",
    "- A Residual VQ demo showing iterative refinement\n",
    "- Encode/decode Vietnamese audio with NeuCodec and DistillNeuCodec\n",
    "- Measure reconstruction quality with SI-SNR and MCD\n",
    "- Compute token budgets for Vietnamese TTS context windows\n",
    "\n",
    "**Prerequisites:** Chapter 3 (audio features), basic PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch04-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nimport soundfile as sf\nimport torch\n\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"librosa: {librosa.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\n# Try importing neucodec (may not be installed in all environments)\ntry:\n    from neucodec import NeuCodec, DistillNeuCodec\n    NEUCODEC_AVAILABLE = True\n    print(\"neucodec: available\")\nexcept ImportError:\n    NEUCODEC_AVAILABLE = False\n    print(\"neucodec: NOT installed — codec sections will be skipped. Install with: pip install neucodec\")\n\n# ── Path resolver (works regardless of Jupyter CWD) ──────────────\nfrom pathlib import Path\nimport os as _os\n\ndef _find_examples_dir():\n    # Walk up from CWD (works locally when kernel starts in chapters/)\n    for _p in [Path(_os.getcwd())] + list(Path(_os.getcwd()).parents):\n        _d = _p / \"examples\" / \"audio_ref\"\n        if _d.is_dir():\n            return _d\n    # Colab fallback paths\n    for _candidate in [\n        Path(\"/content/vietnamese-tts-course/examples/audio_ref\"),\n        Path(\"/content/VieNeu-TTS/examples/audio_ref\"),\n    ]:\n        if _candidate.is_dir():\n            return _candidate\n    raise FileNotFoundError(\n        \"examples/audio_ref/ not found. \"\n        \"Clone the repo: git clone https://github.com/thinhdanggroup/vietnamese-tts-course.git\"\n    )\n\nEXAMPLES_DIR = _find_examples_dir()\nprint(f\"Examples dir: {EXAMPLES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch04-vq-header",
   "metadata": {},
   "source": [
    "## 1. Vector Quantization — From Scratch\n",
    "\n",
    "Before using a real codec, let's build intuition by implementing VQ in 2D. This lets us visualize exactly what quantization does.\n",
    "\n",
    "**Setup:**\n",
    "- Simulate 200 encoder output vectors in $\\mathbb{R}^2$\n",
    "- Codebook: $K=8$ entries, each in $\\mathbb{R}^2$\n",
    "- Quantize: assign each vector to nearest codebook entry\n",
    "- Visualize assignments and residuals\n",
    "\n",
    "**Key math:**\n",
    "$$k^* = \\arg\\min_{k} \\|z_e - e_k\\|_2^2$$\n",
    "$$z_q = e_{k^*}$$\n",
    "$$\\text{residual} = z_e - z_q$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch04-vq-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate encoder output: 200 2D vectors\n",
    "# In a real codec, these would be the bottleneck features of the CNN encoder\n",
    "z_e = np.random.randn(200, 2)\n",
    "\n",
    "# Codebook: K=8 vectors, each D=2 dimensional\n",
    "K = 8\n",
    "codebook = np.random.randn(K, 2)\n",
    "\n",
    "def vq(z_e, codebook):\n",
    "    \"\"\"\n",
    "    Vector Quantization.\n",
    "    \n",
    "    Args:\n",
    "        z_e: (N, D) encoder outputs\n",
    "        codebook: (K, D) codebook entries\n",
    "    \n",
    "    Returns:\n",
    "        z_q: (N, D) quantized vectors\n",
    "        indices: (N,) integer token indices\n",
    "    \"\"\"\n",
    "    # Compute pairwise distances: (N, K)\n",
    "    # Broadcasting: z_e[:, None, :] is (N,1,D), codebook[None,:,:] is (1,K,D)\n",
    "    distances = np.linalg.norm(z_e[:, None, :] - codebook[None, :, :], axis=2)\n",
    "    \n",
    "    # Assign each vector to nearest codebook entry\n",
    "    indices = np.argmin(distances, axis=1)  # shape: (N,)\n",
    "    \n",
    "    # Look up quantized vectors\n",
    "    z_q = codebook[indices]  # shape: (N, D)\n",
    "    \n",
    "    return z_q, indices\n",
    "\n",
    "z_q, indices = vq(z_e, codebook)\n",
    "residuals = z_e - z_q\n",
    "\n",
    "print(f\"Encoder output z_e shape: {z_e.shape}\")\n",
    "print(f\"Codebook shape:           {codebook.shape}\")\n",
    "print(f\"Quantized z_q shape:      {z_q.shape}\")\n",
    "print(f\"\\nToken indices (first 20): {indices[:20].tolist()}\")\n",
    "print(f\"Unique tokens used: {len(np.unique(indices))} / {K}\")\n",
    "print(f\"\\nMean quantization error: {np.mean(np.linalg.norm(residuals, axis=1)):.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = plt.cm.Set1(indices / K)\n",
    "\n",
    "# Left: original encoder outputs with codebook\n",
    "axes[0].scatter(z_e[:, 0], z_e[:, 1], c=colors, alpha=0.5, s=30, label=\"Encoder output z_e\")\n",
    "axes[0].scatter(codebook[:, 0], codebook[:, 1], c='black', s=300, marker='*',\n",
    "                zorder=5, label=\"Codebook entries\")\n",
    "# Draw lines from each point to its codebook entry (sample 20 for clarity)\n",
    "for i in range(0, 200, 10):\n",
    "    axes[0].plot([z_e[i, 0], codebook[indices[i], 0]],\n",
    "                 [z_e[i, 1], codebook[indices[i], 1]],\n",
    "                 'gray', alpha=0.3, linewidth=0.7)\n",
    "axes[0].set_title(\"VQ: Each point assigned to nearest codebook vector\", fontsize=11)\n",
    "axes[0].set_xlabel(\"Latent dimension 1\")\n",
    "axes[0].set_ylabel(\"Latent dimension 2\")\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Right: residuals (what RVQ would quantize next)\n",
    "axes[1].scatter(residuals[:, 0], residuals[:, 1], c=colors, alpha=0.5, s=30)\n",
    "axes[1].axhline(0, color='black', linewidth=0.5)\n",
    "axes[1].axvline(0, color='black', linewidth=0.5)\n",
    "axes[1].set_title(\"Residuals after VQ\\n(what RVQ Stage 2 would quantize next)\", fontsize=11)\n",
    "axes[1].set_xlabel(\"Residual dimension 1\")\n",
    "axes[1].set_ylabel(\"Residual dimension 2\")\n",
    "\n",
    "# Show residual magnitude distribution\n",
    "magnitudes = np.linalg.norm(residuals, axis=1)\n",
    "axes[1].set_title(\n",
    "    f\"Residuals after VQ\\nMean residual: {magnitudes.mean():.3f}, Max: {magnitudes.max():.3f}\",\n",
    "    fontsize=10\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"vq_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: vq_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch04-rvq-header",
   "metadata": {},
   "source": [
    "## 2. Residual VQ — Iterative Refinement\n",
    "\n",
    "A single VQ stage leaves a large residual. RVQ applies VQ repeatedly to the residuals, each time reducing the error.\n",
    "\n",
    "**Algorithm:**\n",
    "$$z_0 = z_e$$\n",
    "For $i = 1, \\ldots, Q$:\n",
    "$$k_i^* = \\arg\\min_k \\|z_{i-1} - e_k^{(i)}\\|_2^2, \\quad q_i = e_{k_i^*}^{(i)}, \\quad z_i = z_{i-1} - q_i$$\n",
    "\n",
    "Final reconstruction: $z_q = \\sum_{i=1}^{Q} q_i$\n",
    "\n",
    "Watch how reconstruction error decreases with each stage, and how the residual becomes progressively smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch04-rvq-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Same encoder outputs as before\n",
    "z_e_rvq = np.random.randn(200, 2)\n",
    "\n",
    "# RVQ: 4 stages, each with K=8 codebook entries\n",
    "Q = 4  # number of quantization stages\n",
    "K_rvq = 8\n",
    "\n",
    "# Initialize separate codebook per stage\n",
    "codebooks = [np.random.randn(K_rvq, 2) * 0.8 for _ in range(Q)]\n",
    "\n",
    "def rvq(z_e, codebooks):\n",
    "    \"\"\"\n",
    "    Residual Vector Quantization.\n",
    "    \n",
    "    Returns:\n",
    "        quantized: list of (z_q_cumulative, residual) per stage\n",
    "        all_indices: (N, Q) token indices per stage\n",
    "    \"\"\"\n",
    "    Q = len(codebooks)\n",
    "    residual = z_e.copy()\n",
    "    cumulative_q = np.zeros_like(z_e)\n",
    "    \n",
    "    results = []\n",
    "    all_indices = []\n",
    "    \n",
    "    for i, cb in enumerate(codebooks):\n",
    "        # Quantize residual\n",
    "        distances = np.linalg.norm(residual[:, None, :] - cb[None, :, :], axis=2)\n",
    "        indices = np.argmin(distances, axis=1)\n",
    "        q = cb[indices]\n",
    "        \n",
    "        # Update\n",
    "        cumulative_q = cumulative_q + q\n",
    "        residual = residual - q\n",
    "        \n",
    "        all_indices.append(indices)\n",
    "        results.append({\n",
    "            'stage': i + 1,\n",
    "            'z_q_cumulative': cumulative_q.copy(),\n",
    "            'residual': residual.copy(),\n",
    "            'error': np.mean(np.linalg.norm(z_e - cumulative_q, axis=1))\n",
    "        })\n",
    "    \n",
    "    return results, np.stack(all_indices, axis=1)\n",
    "\n",
    "results, all_indices = rvq(z_e_rvq, codebooks)\n",
    "\n",
    "print(\"RVQ Reconstruction Error by Stage:\")\n",
    "print(f\"  Stage 0 (no quantization): {np.mean(np.linalg.norm(z_e_rvq, axis=1)):.4f} (original magnitude)\")\n",
    "for r in results:\n",
    "    print(f\"  Stage {r['stage']} (cumulative):   mean error = {r['error']:.4f}\")\n",
    "\n",
    "print(f\"\\nToken indices shape: {all_indices.shape}  (N samples × Q stages)\")\n",
    "print(f\"Each sample represented by: {Q} integers\")\n",
    "print(f\"Effective codebook size: {K_rvq}^{Q} = {K_rvq**Q} (but only {Q * K_rvq} stored)\")\n",
    "\n",
    "# Visualize reconstruction quality across stages\n",
    "fig, axes = plt.subplots(1, Q + 1, figsize=(4 * (Q + 1), 4))\n",
    "\n",
    "# Original\n",
    "axes[0].scatter(z_e_rvq[:, 0], z_e_rvq[:, 1], alpha=0.4, s=20, color='steelblue')\n",
    "axes[0].set_title(\"Original z_e\", fontsize=9)\n",
    "axes[0].set_xlim(-3, 3); axes[0].set_ylim(-3, 3)\n",
    "\n",
    "# Each stage's cumulative reconstruction\n",
    "for i, r in enumerate(results):\n",
    "    ax = axes[i + 1]\n",
    "    colors = plt.cm.Set1(all_indices[:, 0] / K_rvq)  # color by stage-1 token\n",
    "    ax.scatter(r['z_q_cumulative'][:, 0], r['z_q_cumulative'][:, 1],\n",
    "               alpha=0.4, s=20, c=colors)\n",
    "    ax.set_title(f\"After {r['stage']} VQ stage{'s' if r['stage'] > 1 else ''}\\nError: {r['error']:.4f}\",\n",
    "                 fontsize=9)\n",
    "    ax.set_xlim(-3, 3); ax.set_ylim(-3, 3)\n",
    "\n",
    "plt.suptitle(\"RVQ: Cumulative Reconstruction Quality per Stage\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rvq_stages.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot error curve\n",
    "fig2, ax2 = plt.subplots(figsize=(7, 4))\n",
    "errors = [r['error'] for r in results]\n",
    "ax2.plot(range(1, Q + 1), errors, 'o-', linewidth=2, markersize=8, color='crimson')\n",
    "ax2.fill_between(range(1, Q + 1), errors, alpha=0.2, color='crimson')\n",
    "ax2.set_xlabel(\"Number of VQ stages used\", fontsize=11)\n",
    "ax2.set_ylabel(\"Mean reconstruction error (L2)\", fontsize=11)\n",
    "ax2.set_title(\"RVQ: Error Reduction per Quantization Stage\", fontsize=12)\n",
    "ax2.set_xticks(range(1, Q + 1))\n",
    "ax2.set_xticklabels([f\"Stage {i}\" for i in range(1, Q + 1)])\n",
    "ax2.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rvq_error_curve.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nKey insight: diminishing returns — each stage adds less improvement.\")\n",
    "print(\"DistillNeuCodec uses Q=1 but compensates with distillation from a Q=8 teacher.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch04-encode-header",
   "metadata": {},
   "source": [
    "## 3. Encoding a Vietnamese Audio Sample with NeuCodec\n",
    "\n",
    "Now we use the real NeuCodec to encode a Vietnamese audio file. We will:\n",
    "1. Load a `.wav` file from the `examples/audio_ref/` directory\n",
    "2. Encode it to discrete integer tokens\n",
    "3. Inspect the token sequence shape and rate\n",
    "\n",
    "**Expected output:**\n",
    "- For a 3-second clip at 24 kHz: shape `[1, 1, 150]` (1 batch × 1 codebook level × 150 frames)\n",
    "- Token rate: 50 tokens/sec\n",
    "\n",
    "> If `neucodec` is not installed, the cell will display a message explaining what the output would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch04-encode-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport librosa\nimport numpy as np\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\naudio_path = str(EXAMPLES_DIR / \"example.wav\")\n\nif not NEUCODEC_AVAILABLE:\n    print(\"\\n[DEMO MODE] neucodec not installed — showing expected output structure.\")\n    print(\"Install with: pip install neucodec\")\n    print(\"\\nExpected output for a 3-second clip:\")\n    print(\"  Input shape: torch.Size([1, 1, 48000])  (batch=1, channels=1, samples=3s×16000)\")\n    print(\"  Encoded codes shape: torch.Size([1, 1, 150])\")\n    print(\"  Token rate: 50.0 tokens/sec\")\n    print(\"  First 20 tokens: [234, 891, 45, 12, 567, 88, 203, 445, 91, 332, 178, 5, 612, 289, 401, 73, 156, 799, 22, 487]\")\nelse:\n    # Load DistillNeuCodec (single-codebook, 50 tok/sec)\n    codec = DistillNeuCodec.from_pretrained(\"neuphonic/distill-neucodec\")\n    codec.eval().to(device)\n    print(\"DistillNeuCodec loaded.\")\n\n    # Load audio at 16kHz (codec will handle resampling internally)\n    wav, sr = librosa.load(audio_path, sr=16000, mono=True)\n    wav_tensor = torch.from_numpy(wav).float().unsqueeze(0).unsqueeze(0).to(device)\n\n    print(f\"\\nInput shape: {wav_tensor.shape}\")\n    print(f\"Input duration: {wav.shape[0]/sr:.2f}s at {sr} Hz\")\n\n    with torch.no_grad():\n        codes = codec.encode_code(wav_tensor)\n\n    print(f\"\\nEncoded codes shape: {codes.shape}\")\n    duration_s = wav.shape[0] / sr\n    # codes shape is typically [batch, codebook_levels, n_frames]\n    n_frames = codes.shape[-1]\n    print(f\"Token rate: {n_frames / duration_s:.1f} tokens/sec\")\n    print(f\"First 20 tokens: {codes.squeeze()[:20].tolist()}\")\n    print(f\"Token range: [{codes.min().item()}, {codes.max().item()}]\")\n    print(f\"Unique tokens used: {codes.unique().shape[0]}\")\n\nprint(\"\\n--- Context window analysis ---\")\ntoken_rate = 50\nfor duration in [1, 2, 3, 5, 10]:\n    tokens = token_rate * duration\n    print(f\"  {duration:2d}s audio → {tokens:4d} speech tokens  \"\n          f\"({tokens/2048*100:.1f}% of 2048-token context)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch04-decode-header",
   "metadata": {},
   "source": [
    "## 4. Decoding Back to Audio\n",
    "\n",
    "The codec decoder reconstructs a waveform from the discrete token sequence. We will:\n",
    "1. Decode the encoded tokens\n",
    "2. Play both original and reconstructed audio side by side\n",
    "3. Visually compare waveforms and spectrograms\n",
    "\n",
    "Even with just 50 tokens/sec, a well-trained codec reconstructs speech that sounds very close to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch04-decode-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load original for comparison (regardless of neucodec availability)\n",
    "wav_orig, sr_orig = librosa.load(audio_path, sr=None, mono=True)\n",
    "print(f\"Original: {len(wav_orig)} samples at {sr_orig} Hz = {len(wav_orig)/sr_orig:.2f}s\")\n",
    "\n",
    "if NEUCODEC_AVAILABLE:\n",
    "    with torch.no_grad():\n",
    "        reconstructed_tensor = codec.decode_code(codes)\n",
    "\n",
    "    reconstructed_np = reconstructed_tensor.squeeze().cpu().numpy()\n",
    "    sr_recon = 24000  # DistillNeuCodec output sample rate\n",
    "    print(f\"Reconstructed: {len(reconstructed_np)} samples at {sr_recon} Hz = {len(reconstructed_np)/sr_recon:.2f}s\")\n",
    "\n",
    "    from IPython.display import Audio, display\n",
    "    print(\"\\nOriginal audio:\")\n",
    "    display(Audio(wav_orig, rate=sr_orig))\n",
    "    print(\"Reconstructed from codec tokens (DistillNeuCodec):\")\n",
    "    display(Audio(reconstructed_np, rate=sr_recon))\n",
    "\n",
    "    # Visual comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "    # Waveforms\n",
    "    t_orig = np.linspace(0, len(wav_orig)/sr_orig, len(wav_orig))\n",
    "    t_recon = np.linspace(0, len(reconstructed_np)/sr_recon, len(reconstructed_np))\n",
    "\n",
    "    axes[0, 0].plot(t_orig, wav_orig, linewidth=0.5, color='steelblue')\n",
    "    axes[0, 0].set_title(\"Original Waveform\", fontsize=11)\n",
    "    axes[0, 0].set_xlabel(\"Time (s)\")\n",
    "\n",
    "    axes[0, 1].plot(t_recon, reconstructed_np, linewidth=0.5, color='crimson')\n",
    "    axes[0, 1].set_title(\"Reconstructed Waveform (DistillNeuCodec)\", fontsize=11)\n",
    "    axes[0, 1].set_xlabel(\"Time (s)\")\n",
    "\n",
    "    # Spectrograms\n",
    "    D_orig = librosa.stft(wav_orig, n_fft=1024, hop_length=256)\n",
    "    D_recon = librosa.stft(reconstructed_np, n_fft=1024, hop_length=256)\n",
    "\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(D_orig), ref=np.max),\n",
    "                             sr=sr_orig, hop_length=256, y_axis='log', x_axis='time',\n",
    "                             ax=axes[1, 0], cmap='magma')\n",
    "    axes[1, 0].set_title(\"Original Spectrogram\", fontsize=11)\n",
    "\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(D_recon), ref=np.max),\n",
    "                             sr=sr_recon, hop_length=256, y_axis='log', x_axis='time',\n",
    "                             ax=axes[1, 1], cmap='magma')\n",
    "    axes[1, 1].set_title(\"Reconstructed Spectrogram\", fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"codec_reconstruction_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[DEMO MODE] Showing original audio spectrogram only (neucodec not available)\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    D = librosa.stft(wav_orig, n_fft=1024, hop_length=256)\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(D), ref=np.max),\n",
    "                             sr=sr_orig, hop_length=256, y_axis='log', x_axis='time',\n",
    "                             ax=ax, cmap='magma')\n",
    "    ax.set_title(\"Original Audio Spectrogram (Vietnamese example)\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch04-metrics-header",
   "metadata": {},
   "source": [
    "## 5. Measuring Reconstruction Quality\n",
    "\n",
    "We implement two codec quality metrics from scratch:\n",
    "\n",
    "**SI-SNR (Scale-Invariant Signal-to-Noise Ratio):**\n",
    "$$s_\\text{target} = \\frac{\\langle \\hat{s}, s \\rangle}{\\|s\\|^2} s, \\quad e_\\text{noise} = \\hat{s} - s_\\text{target}$$\n",
    "$$\\text{SI-SNR} = 10 \\log_{10} \\frac{\\|s_\\text{target}\\|^2}{\\|e_\\text{noise}\\|^2}$$\n",
    "\n",
    "**MCD (Mel Cepstral Distortion):**\n",
    "$$\\text{MCD} = \\frac{10}{\\ln 10} \\sqrt{2 \\sum_{k=1}^{K} (c_k^\\text{ref} - c_k^\\text{syn})^2} \\quad [\\text{dB}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch04-metrics-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def si_snr(target: np.ndarray, estimate: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Scale-Invariant Signal-to-Noise Ratio (SI-SNR).\n",
    "    Both signals must be 1D arrays. Length is aligned automatically.\n",
    "    \n",
    "    Higher is better. Typical codec: 20-30 dB.\n",
    "    \"\"\"\n",
    "    # Remove DC (zero-mean)\n",
    "    target = target - np.mean(target)\n",
    "    estimate = estimate - np.mean(estimate)\n",
    "    \n",
    "    # Align lengths\n",
    "    min_len = min(len(target), len(estimate))\n",
    "    target = target[:min_len]\n",
    "    estimate = estimate[:min_len]\n",
    "    \n",
    "    # Project estimate onto target (scale-invariant scaling)\n",
    "    alpha = np.dot(estimate, target) / (np.dot(target, target) + 1e-8)\n",
    "    s_target = alpha * target\n",
    "    \n",
    "    # Noise\n",
    "    e_noise = estimate - s_target\n",
    "    \n",
    "    # SI-SNR in dB\n",
    "    return 10 * np.log10(\n",
    "        np.dot(s_target, s_target) / (np.dot(e_noise, e_noise) + 1e-8)\n",
    "    )\n",
    "\n",
    "\n",
    "def mcd(ref_wav: np.ndarray, syn_wav: np.ndarray, sr: int = 24000, n_mfcc: int = 13) -> float:\n",
    "    \"\"\"\n",
    "    Mel Cepstral Distortion (MCD) in dB.\n",
    "    Lower is better. Good codec: < 5 dB.\n",
    "    \"\"\"\n",
    "    ref_mfcc = librosa.feature.mfcc(y=ref_wav, sr=sr, n_mfcc=n_mfcc)\n",
    "    syn_mfcc = librosa.feature.mfcc(y=syn_wav, sr=sr, n_mfcc=n_mfcc)\n",
    "    \n",
    "    # Align frame lengths\n",
    "    min_len = min(ref_mfcc.shape[1], syn_mfcc.shape[1])\n",
    "    ref_mfcc = ref_mfcc[:, :min_len]\n",
    "    syn_mfcc = syn_mfcc[:, :min_len]\n",
    "    \n",
    "    # MCD formula (skip coefficient 0 which is energy, matches standard)\n",
    "    diff = ref_mfcc[1:, :] - syn_mfcc[1:, :]  # shape: (n_mfcc-1, T)\n",
    "    frame_mcd = np.sqrt(2 * np.sum(diff**2, axis=0))  # per-frame MCD\n",
    "    return float(np.mean(frame_mcd) * (10 / np.log(10)))\n",
    "\n",
    "\n",
    "if NEUCODEC_AVAILABLE:\n",
    "    # Resample original to 24kHz for fair comparison with reconstructed\n",
    "    wav_24k = librosa.resample(wav_orig, orig_sr=sr_orig, target_sr=24000)\n",
    "\n",
    "    snr = si_snr(wav_24k, reconstructed_np)\n",
    "    mcd_val = mcd(wav_24k, reconstructed_np, sr=24000)\n",
    "\n",
    "    print(\"=== Reconstruction Quality Metrics ===\")\n",
    "    print(f\"SI-SNR: {snr:.2f} dB  (higher is better; >20 dB = good)\")\n",
    "    print(f\"MCD:    {mcd_val:.2f} dB  (lower is better; <5 dB = good)\")\n",
    "\n",
    "    print(\"\\n=== Reference Values ===\")\n",
    "    print(\"  SI-SNR > 25 dB: excellent (near transparent)\")\n",
    "    print(\"  SI-SNR 20-25 dB: good\")\n",
    "    print(\"  SI-SNR < 15 dB: noticeable artifacts\")\n",
    "    print(\"  MCD < 3 dB: very good spectral match\")\n",
    "    print(\"  MCD 3-6 dB: acceptable\")\n",
    "    print(\"  MCD > 6 dB: degraded\")\n",
    "else:\n",
    "    print(\"[DEMO MODE] Showing what the metrics measure on synthetic data.\")\n",
    "    \n",
    "    # Create a synthetic example: original vs slightly noisy version\n",
    "    np.random.seed(0)\n",
    "    t = np.linspace(0, 1, 24000)\n",
    "    original = 0.5 * np.sin(2 * np.pi * 200 * t) + 0.3 * np.sin(2 * np.pi * 400 * t)\n",
    "    \n",
    "    noise_levels = [0.0, 0.01, 0.05, 0.1, 0.3]\n",
    "    print(\"\\nEffect of noise level on SI-SNR:\")\n",
    "    for noise_std in noise_levels:\n",
    "        noisy = original + np.random.randn(len(original)) * noise_std\n",
    "        snr_val = si_snr(original, noisy)\n",
    "        print(f\"  Noise std={noise_std:.2f}: SI-SNR = {snr_val:.1f} dB\")\n",
    "    \n",
    "    print(\"\\nIn practice, DistillNeuCodec achieves approximately:\")\n",
    "    print(\"  SI-SNR: ~22-27 dB on clean Vietnamese speech\")\n",
    "    print(\"  MCD:    ~3.5-5.0 dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch04-distill-header",
   "metadata": {},
   "source": [
    "## 6. DistillNeuCodec vs NeuCodec — Token Rate Comparison\n",
    "\n",
    "We compare the two codecs directly:\n",
    "\n",
    "| Codec | Q | Frames/sec | Tokens/sec |\n",
    "|---|---|---|---|\n",
    "| NeuCodec (RVQ) | 8 | 75 | **600** |\n",
    "| DistillNeuCodec | 1 | 50 | **50** |\n",
    "\n",
    "The 12x reduction in tokens/sec is what makes LLM-based TTS feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch04-distill-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "\n",
    "if NEUCODEC_AVAILABLE:\n",
    "    # Load NeuCodec (full RVQ)\n",
    "    neu_codec = NeuCodec.from_pretrained(\"neuphonic/neucodec\")\n",
    "    neu_codec.eval().to(device)\n",
    "\n",
    "    wav_cmp, sr_cmp = librosa.load(audio_path, sr=16000, mono=True)\n",
    "    wav_t = torch.from_numpy(wav_cmp).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "    duration_s = wav_cmp.shape[0] / sr_cmp\n",
    "\n",
    "    with torch.no_grad():\n",
    "        neu_codes = neu_codec.encode_code(wav_t)\n",
    "        distill_codes_cmp = distill_codec.encode_code(wav_t)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(\"NeuCodec (full RVQ, Q=8):\")\n",
    "    print(f\"  Code shape: {neu_codes.shape}\")\n",
    "    print(f\"  Token rate: {neu_codes.shape[-1] / duration_s:.1f} tokens/sec\")\n",
    "    print(f\"  Total tokens for {duration_s:.1f}s: {neu_codes.shape[-1] * neu_codes.shape[1]}\")\n",
    "    print()\n",
    "    print(\"DistillNeuCodec (single codebook, Q=1):\")\n",
    "    print(f\"  Code shape: {distill_codes_cmp.shape}\")\n",
    "    print(f\"  Token rate: {distill_codes_cmp.shape[-1] / duration_s:.1f} tokens/sec\")\n",
    "    print(f\"  Total tokens for {duration_s:.1f}s: {distill_codes_cmp.shape[-1]}\")\n",
    "    print(\"=\" * 50)\n",
    "    reduction = (neu_codes.shape[-1] * neu_codes.shape[1]) / distill_codes_cmp.shape[-1]\n",
    "    print(f\"\\nToken reduction factor: {reduction:.1f}x\")\n",
    "else:\n",
    "    print(\"[DEMO MODE] Showing expected comparison results:\")\n",
    "    print()\n",
    "    duration_s = 3.0  # example 3-second clip\n",
    "    print(\"=\" * 50)\n",
    "    print(\"NeuCodec (full RVQ, Q=8):\")\n",
    "    print(f\"  Code shape: torch.Size([1, 8, {int(75*duration_s)}])\")\n",
    "    print(f\"  Token rate: 75.0 frames/sec × 8 codebooks = 600 tokens/sec\")\n",
    "    print(f\"  Total tokens for {duration_s:.1f}s: {int(600*duration_s)}\")\n",
    "    print()\n",
    "    print(\"DistillNeuCodec (single codebook, Q=1):\")\n",
    "    print(f\"  Code shape: torch.Size([1, 1, {int(50*duration_s)}])\")\n",
    "    print(f\"  Token rate: 50.0 tokens/sec\")\n",
    "    print(f\"  Total tokens for {duration_s:.1f}s: {int(50*duration_s)}\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"\\nToken reduction factor: {600/50:.1f}x\")\n",
    "\n",
    "# Bar chart comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "codecs = ['NeuCodec\\n(RVQ Q=8)\\n600 tok/sec', 'DistillNeuCodec\\n(Q=1)\\n50 tok/sec']\n",
    "rates = [600, 50]\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "bars = ax.bar(codecs, rates, color=colors, width=0.4, edgecolor='black')\n",
    "for bar, rate in zip(bars, rates):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "            f'{rate}\\ntokens/sec', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel(\"Tokens per second\", fontsize=11)\n",
    "ax.set_title(\"Token Rate: NeuCodec vs DistillNeuCodec\\n(lower = easier LLM modeling, longer context)\",\n",
    "             fontsize=12)\n",
    "ax.set_ylim(0, 700)\n",
    "ax.axhline(2048 / 40, color='orange', linestyle='--', linewidth=1.5,\n",
    "           label='~51 tok/sec: max rate for 40s in 2048-token context')\n",
    "ax.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"token_rate_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch04-budget-header",
   "metadata": {},
   "source": [
    "## 7. Token Count for Vietnamese TTS Context Window\n",
    "\n",
    "Let's calculate exactly how the 2,048-token context window is used in VieNeu-TTS.\n",
    "\n",
    "**Prompt structure:**\n",
    "```\n",
    "[system tokens] + [text tokens] + [ref_codes] + [SPEECH_START] + [generated codes]\n",
    "```\n",
    "\n",
    "Vietnamese sentence statistics (from Vietnamese speech corpora):\n",
    "- Average syllables per second: ~5-6\n",
    "- Average sentence duration: 3-5 seconds\n",
    "- At 50 tokens/sec: 150-250 tokens per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch04-budget-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "CONTEXT_SIZE = 2048\n",
    "TOKEN_RATE = 50  # tokens/sec for DistillNeuCodec\n",
    "\n",
    "# Fixed prompt components\n",
    "SYSTEM_TOKENS = 20      # \"Convert the text to speech:\" etc.\n",
    "SPECIAL_TOKENS = 4      # TEXT_START, TEXT_END, SPEECH_START, SPEECH_END\n",
    "\n",
    "# Variable components\n",
    "ref_duration_s = 3.0\n",
    "ref_text_tokens = 30    # phoneme tokens for reference transcript\n",
    "input_text_tokens = 40  # phoneme tokens for target text\n",
    "\n",
    "ref_speech_tokens = int(TOKEN_RATE * ref_duration_s)\n",
    "\n",
    "used_tokens = SYSTEM_TOKENS + SPECIAL_TOKENS + ref_text_tokens + input_text_tokens + ref_speech_tokens\n",
    "remaining_for_generation = CONTEXT_SIZE - used_tokens\n",
    "max_gen_duration_s = remaining_for_generation / TOKEN_RATE\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"VieNeu-TTS Context Window Budget (2048 tokens)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"System + special tokens:  {SYSTEM_TOKENS + SPECIAL_TOKENS:4d} tokens\")\n",
    "print(f\"Reference text (phonemes): {ref_text_tokens:4d} tokens\")\n",
    "print(f\"Input text (phonemes):     {input_text_tokens:4d} tokens\")\n",
    "print(f\"Reference speech codes:    {ref_speech_tokens:4d} tokens  ({ref_duration_s}s × {TOKEN_RATE} tok/s)\")\n",
    "print(f\"  — — — — — — — — — — — — — — — — — —\")\n",
    "print(f\"Total used (prompt):      {used_tokens:4d} tokens  ({used_tokens/CONTEXT_SIZE*100:.1f}% of context)\")\n",
    "print(f\"Available for generation: {remaining_for_generation:4d} tokens\")\n",
    "print(f\"Max generatable speech:   {max_gen_duration_s:.1f}s\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Typical Vietnamese sentences\n",
    "print(\"\\nTypical Vietnamese utterance token costs:\")\n",
    "sentences = [\n",
    "    (\"Xin chào.\", 1.0),\n",
    "    (\"Hôm nay trời đẹp.\", 2.0),\n",
    "    (\"AI đang thay đổi cuộc sống.\", 3.5),\n",
    "    (\"Mô hình học sâu được ứng dụng rộng rãi trong nhiều lĩnh vực.\", 5.0),\n",
    "    (\"Tiếng Việt có sáu thanh điệu và nhiều phương ngữ vùng miền.\", 6.0),\n",
    "]\n",
    "for text, dur_s in sentences:\n",
    "    tokens = int(TOKEN_RATE * dur_s)\n",
    "    pct_remaining = tokens / remaining_for_generation * 100\n",
    "    print(f\"  {dur_s:.1f}s | {tokens:3d} tokens ({pct_remaining:.0f}% of budget) | {text[:50]}\")\n",
    "\n",
    "# Stacked bar chart of context usage\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "\n",
    "components = [\n",
    "    (\"System + Special\\n(24 tokens)\", SYSTEM_TOKENS + SPECIAL_TOKENS, '#95a5a6'),\n",
    "    (\"Ref text\\n(30 tokens)\", ref_text_tokens, '#3498db'),\n",
    "    (\"Input text\\n(40 tokens)\", input_text_tokens, '#2ecc71'),\n",
    "    (\"Ref speech codes\\n(150 tokens, 3s)\", ref_speech_tokens, '#e67e22'),\n",
    "    (\"Available for generated speech\\n(1804 tokens = 36s max)\", remaining_for_generation, '#e8f5e9'),\n",
    "]\n",
    "\n",
    "left = 0\n",
    "patches = []\n",
    "for label, width, color in components:\n",
    "    ax.barh(0, width, left=left, color=color, edgecolor='white', height=0.5)\n",
    "    if width > 50:\n",
    "        ax.text(left + width/2, 0, f'{width}', ha='center', va='center',\n",
    "                fontsize=9, fontweight='bold')\n",
    "    left += width\n",
    "    patches.append(mpatches.Patch(color=color, label=label))\n",
    "\n",
    "ax.set_xlim(0, CONTEXT_SIZE)\n",
    "ax.set_ylim(-0.5, 0.5)\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(\"Token position in 2048-token context\", fontsize=11)\n",
    "ax.set_title(\"VieNeu-TTS: Context Window Token Budget\", fontsize=12)\n",
    "ax.legend(handles=patches, loc='upper right', fontsize=7, ncol=5,\n",
    "          bbox_to_anchor=(1.0, 1.6))\n",
    "ax.axvline(CONTEXT_SIZE, color='red', linewidth=2, linestyle='--')\n",
    "ax.text(CONTEXT_SIZE - 10, 0.3, '2048 limit', ha='right', color='red', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"context_budget.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey takeaway: With DistillNeuCodec at 50 tokens/sec,\")\n",
    "print(\"a typical Vietnamese TTS request uses only ~12% of the context window.\")\n",
    "print(\"The remaining 88% is available for generating speech — up to 36 seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch04-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Concept | Key Formula | For VieNeu-TTS |\n",
    "|---|---|---|\n",
    "| VQ encoding | $k^* = \\arg\\min_k \\|z_e - e_k\\|_2^2$ | Maps audio frames to integers |\n",
    "| Commitment loss | $\\beta\\|z_e - \\text{sg}[e_{k^*}]\\|_2^2$ | Prevents encoder from drifting |\n",
    "| RVQ | $z_q = \\sum_{i=1}^Q q_i$ | NeuCodec uses Q=8 stages |\n",
    "| Token rate | frames/sec × Q | DistillNeuCodec: 50×1 = **50 tok/sec** |\n",
    "| Context budget (3s ref) | 2048 − 244 prompt tokens | **1804 tokens** for generation = 36s max |\n",
    "| SI-SNR | $10\\log_{10}(\\|s_t\\|^2/\\|e_n\\|^2)$ | Measures reconstruction fidelity |\n",
    "\n",
    "**Next chapter:** How VieNeu-TTS uses these tokens as input and output of a causal language model to generate speech autoregressively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ch06-title",
   "metadata": {},
   "source": [
    "# Chapter 6 — Zero-Shot Voice Cloning\n",
    "\n",
    "**Goal:** Understand how VieNeu-TTS clones any voice with just a 3-5 second reference clip — no fine-tuning required.\n",
    "\n",
    "**What you will do:**\n",
    "1. Visualize voice identity in frequency space — see that different voices are acoustically distinct\n",
    "2. Encode multiple Vietnamese reference voices as codec token sequences\n",
    "3. Clone three different Vietnamese voices saying the same target sentence\n",
    "4. Measure speaker similarity with cosine distance\n",
    "5. Examine how reference length affects clone quality\n",
    "6. Test code-switching (Vietnamese + English) synthesis\n",
    "\n",
    "**Prerequisites:** Chapters 4 (codecs) and 5 (LLM-TTS architecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch06-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nimport soundfile as sf\nimport torch\nfrom IPython.display import Audio, display\n\nsys.path.insert(0, \"..\")\n\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"librosa: {librosa.__version__}\")\n\n# Optional dependencies\ntry:\n    from neucodec import DistillNeuCodec\n    NEUCODEC_AVAILABLE = True\n    print(\"neucodec: available\")\nexcept ImportError:\n    NEUCODEC_AVAILABLE = False\n    print(\"neucodec: NOT installed\")\n\ntry:\n    from vieneu import Vieneu\n    VIENEU_AVAILABLE = True\n    print(\"vieneu: available\")\nexcept ImportError:\n    VIENEU_AVAILABLE = False\n    print(\"vieneu: NOT installed\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"\\nCompute device: {device}\")\n\n# Reference audio files\nREF_FILES = [\n    str(EXAMPLES_DIR / \"example.wav\"),\n    str(EXAMPLES_DIR / \"example_2.wav\"),\n    str(EXAMPLES_DIR / \"example_3.wav\"),\n]\nREF_TEXT_FILES = [\n    str(EXAMPLES_DIR / \"example.txt\"),\n    str(EXAMPLES_DIR / \"example_2.txt\"),\n    str(EXAMPLES_DIR / \"example_3.txt\"),\n]\nprint(f\"\\nReference files available:\")\nfor f in REF_FILES:\n    exists = os.path.exists(f)\n    print(f\"  {'✓' if exists else '✗'} {f}\")\n\n# ── Path resolver (works regardless of Jupyter CWD) ──────────────\nfrom pathlib import Path\nimport os as _os\n\ndef _find_examples_dir():\n    # Walk up from CWD (works locally when kernel starts in chapters/)\n    for _p in [Path(_os.getcwd())] + list(Path(_os.getcwd()).parents):\n        _d = _p / \"examples\" / \"audio_ref\"\n        if _d.is_dir():\n            return _d\n    # Colab fallback paths\n    for _candidate in [\n        Path(\"/content/vietnamese-tts-course/examples/audio_ref\"),\n        Path(\"/content/VieNeu-TTS/examples/audio_ref\"),\n    ]:\n        if _candidate.is_dir():\n            return _candidate\n    raise FileNotFoundError(\n        \"examples/audio_ref/ not found. \"\n        \"Clone the repo: git clone https://github.com/thinhdanggroup/vietnamese-tts-course.git\"\n    )\n\nEXAMPLES_DIR = _find_examples_dir()\nprint(f\"Examples dir: {EXAMPLES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch06-spectrogram-header",
   "metadata": {},
   "source": [
    "## 1. What Does Voice Identity Look Like in Frequency Space?\n",
    "\n",
    "Different speakers have different vocal tract shapes, which produce different formant (resonance) frequencies. These differences are visible in the mel spectrogram:\n",
    "\n",
    "- **Formant 1 (F1):** Related to jaw openness — different in open vs closed vowels\n",
    "- **Formant 2 (F2):** Related to tongue position — front vs back vowels\n",
    "- **Pitch (F0):** Fundamental frequency — higher in female voices\n",
    "- **Spectral envelope:** Overall shape — timbre and voice quality\n",
    "\n",
    "Plotting the mel spectrograms of multiple speakers shows these differences clearly — even when they say similar things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch06-spectrogram-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load reference audios\n",
    "ref_audios = []\n",
    "ref_texts = []\n",
    "\n",
    "for wav_path, txt_path in zip(REF_FILES, REF_TEXT_FILES):\n",
    "    if os.path.exists(wav_path):\n",
    "        wav, sr = librosa.load(wav_path, sr=22050, mono=True)\n",
    "        ref_audios.append((wav, sr, wav_path.split('/')[-1]))\n",
    "    if os.path.exists(txt_path):\n",
    "        ref_texts.append(open(txt_path).read().strip())\n",
    "    else:\n",
    "        ref_texts.append(\"(text not available)\")\n",
    "\n",
    "print(f\"Loaded {len(ref_audios)} reference audio files\")\n",
    "for i, (wav, sr, name) in enumerate(ref_audios):\n",
    "    print(f\"  [{i+1}] {name}: {len(wav)/sr:.2f}s at {sr} Hz\")\n",
    "    if i < len(ref_texts):\n",
    "        print(f\"       Text: {ref_texts[i][:80]}...\")\n",
    "\n",
    "# Plot mel spectrograms side by side\n",
    "n_refs = len(ref_audios)\n",
    "fig, axes = plt.subplots(2, n_refs, figsize=(6 * n_refs, 10))\n",
    "if n_refs == 1:\n",
    "    axes = axes.reshape(2, 1)\n",
    "\n",
    "for i, (wav, sr, name) in enumerate(ref_audios):\n",
    "    # Mel spectrogram\n",
    "    mel = librosa.feature.melspectrogram(y=wav, sr=sr, n_mels=80,\n",
    "                                          hop_length=256, n_fft=1024,\n",
    "                                          fmin=80, fmax=8000)\n",
    "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "    \n",
    "    img = librosa.display.specshow(mel_db, sr=sr, hop_length=256,\n",
    "                                    y_axis='mel', x_axis='time',\n",
    "                                    ax=axes[0, i], cmap='magma',\n",
    "                                    fmin=80, fmax=8000)\n",
    "    axes[0, i].set_title(f\"Speaker {i+1}: {name}\\n80-band Mel Spectrogram\", fontsize=10)\n",
    "    plt.colorbar(img, ax=axes[0, i], format='%+2.0f dB')\n",
    "    \n",
    "    # F0 (pitch) tracking\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(\n",
    "        wav, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'),\n",
    "        sr=sr, hop_length=256\n",
    "    )\n",
    "    times = librosa.times_like(f0, sr=sr, hop_length=256)\n",
    "    \n",
    "    axes[1, i].plot(times[voiced_flag], f0[voiced_flag], 'o', color='#2ecc71',\n",
    "                    markersize=3, alpha=0.7, label='F0 (voiced)')\n",
    "    axes[1, i].set_xlabel(\"Time (s)\", fontsize=9)\n",
    "    axes[1, i].set_ylabel(\"Frequency (Hz)\", fontsize=9)\n",
    "    axes[1, i].set_title(f\"Speaker {i+1} Pitch Contour\", fontsize=10)\n",
    "    axes[1, i].legend(fontsize=8)\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate mean F0\n",
    "    mean_f0 = np.nanmean(f0[voiced_flag]) if voiced_flag.any() else 0\n",
    "    if mean_f0 > 0:\n",
    "        axes[1, i].axhline(mean_f0, color='orange', linestyle='--', linewidth=1,\n",
    "                           label=f'Mean F0: {mean_f0:.0f} Hz')\n",
    "        axes[1, i].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Voice Identity in Frequency Space\\n\"\n",
    "             \"Different speakers → different spectral patterns and pitch ranges\",\n",
    "             fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"voice_spectrograms.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  • Speakers differ in pitch range (higher F0 = higher voice)\")\n",
    "print(\"  • Mel spectrograms show different formant patterns (timbre)\")\n",
    "print(\"  • These differences are encoded in the codec token distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch06-tokens-header",
   "metadata": {},
   "source": [
    "## 2. Encoding Voice Identity as Codec Tokens\n",
    "\n",
    "The DistillNeuCodec encodes each speaker's acoustic characteristics into a sequence of integers. Different speakers produce different token distributions — even for similar content.\n",
    "\n",
    "**Key insight:** The codec token distribution is a fingerprint of the voice. By inserting these tokens into the prompt, the LLM \"inherits\" the voice characteristics for synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch06-tokens-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_codes = []\n",
    "all_names = []\n",
    "\n",
    "if NEUCODEC_AVAILABLE:\n",
    "    codec = DistillNeuCodec.from_pretrained(\"neuphonic/distill-neucodec\").to(device).eval()\n",
    "    \n",
    "    for wav_path, (wav, sr, name) in zip(REF_FILES, ref_audios):\n",
    "        # Resample to 16kHz for codec input\n",
    "        wav_16k = librosa.resample(wav, orig_sr=sr, target_sr=16000)\n",
    "        wav_t = torch.from_numpy(wav_16k).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            codes = codec.encode_code(wav_t).squeeze().cpu().numpy()\n",
    "        \n",
    "        all_codes.append(codes)\n",
    "        all_names.append(name)\n",
    "        print(f\"{name}: {len(codes)} tokens, first 10: {codes[:10].tolist()}\")\n",
    "        print(f\"  Token range: [{codes.min()}, {codes.max()}], \"\n",
    "              f\"unique: {len(np.unique(codes))}, \"\n",
    "              f\"mean: {codes.mean():.0f}\")\n",
    "\n",
    "else:\n",
    "    # Simulate plausible codec codes for demo\n",
    "    print(\"[DEMO MODE] Simulating codec tokens for 3 different voices\")\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Each voice has a different dominant token cluster\n",
    "    for i, (wav, sr, name) in enumerate(ref_audios):\n",
    "        n_tokens = int(50 * len(wav) / sr)\n",
    "        # Different voices cluster in different parts of the codebook\n",
    "        center = [10000, 35000, 55000][i]\n",
    "        codes = np.random.randint(max(0, center - 15000),\n",
    "                                   min(65535, center + 15000),\n",
    "                                   size=n_tokens)\n",
    "        # Add some cross-cluster tokens (phoneme-dependent)\n",
    "        mask = np.random.rand(n_tokens) < 0.15\n",
    "        codes[mask] = np.random.randint(0, 65535, size=mask.sum())\n",
    "        codes = np.clip(codes, 0, 65535).astype(int)\n",
    "        \n",
    "        all_codes.append(codes)\n",
    "        all_names.append(name)\n",
    "        print(f\"{name}: {len(codes)} tokens, center: {center}\")\n",
    "\n",
    "# Visualize token distributions\n",
    "fig, axes = plt.subplots(2, len(all_codes), figsize=(6 * len(all_codes), 9))\n",
    "if len(all_codes) == 1:\n",
    "    axes = axes.reshape(2, 1)\n",
    "\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for i, (codes, name) in enumerate(zip(all_codes, all_names)):\n",
    "    # Token ID histogram\n",
    "    axes[0, i].hist(codes, bins=100, color=colors[i % len(colors)],\n",
    "                     alpha=0.7, edgecolor='none')\n",
    "    axes[0, i].set_title(f\"Speaker {i+1}: {name}\\nToken ID Distribution\", fontsize=10)\n",
    "    axes[0, i].set_xlabel(\"Token ID (0–65535)\", fontsize=9)\n",
    "    axes[0, i].set_ylabel(\"Count\", fontsize=9)\n",
    "    axes[0, i].set_xlim(0, 65535)\n",
    "    \n",
    "    # Token sequence (first 200)\n",
    "    display_len = min(200, len(codes))\n",
    "    axes[1, i].plot(range(display_len), codes[:display_len],\n",
    "                    color=colors[i % len(colors)], linewidth=0.8, alpha=0.8)\n",
    "    axes[1, i].set_title(f\"Speaker {i+1}: Token Sequence (first {display_len})\", fontsize=10)\n",
    "    axes[1, i].set_xlabel(\"Frame index (1 frame = 20ms)\", fontsize=9)\n",
    "    axes[1, i].set_ylabel(\"Token ID\", fontsize=9)\n",
    "    axes[1, i].set_ylim(0, 65535)\n",
    "    axes[1, i].grid(True, alpha=0.2)\n",
    "\n",
    "plt.suptitle(\"Codec Token Distributions — Different Voices Have Different Token Patterns\",\n",
    "             fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"voice_token_distributions.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation:\")\n",
    "print(\"Different speakers concentrate their tokens in different codebook regions.\")\n",
    "print(\"This is the 'voice fingerprint' that the LLM conditions on.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch06-cloning-header",
   "metadata": {},
   "source": [
    "## 3. Cloning Three Vietnamese Voices\n",
    "\n",
    "We synthesize the same target sentence using three different reference voices. The output should:\n",
    "- Say the same words with the same content\n",
    "- Sound like three different people\n",
    "- Match the pitch, timbre, and speaking style of each reference\n",
    "\n",
    "**Target sentence:** \"Xin chào, đây là giọng nói được clone từ file mẫu.\"\n",
    "(\"Hello, this is a voice cloned from the sample file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch06-cloning-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "import librosa\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "target_text = \"Xin chào, đây là giọng nói được clone từ file mẫu.\"\n",
    "\n",
    "print(f\"Target sentence: {target_text}\")\n",
    "print(f\"(Translation: Hello, this is a voice cloned from the sample file.)\")\n",
    "print()\n",
    "\n",
    "if VIENEU_AVAILABLE:\n",
    "    tts = Vieneu()\n",
    "    \n",
    "    cloned_audios = []\n",
    "    \n",
    "    for i, ((wav, sr, name), ref_text) in enumerate(zip(ref_audios, ref_texts)):\n",
    "        ref_audio_path = REF_FILES[i]\n",
    "        \n",
    "        print(f\"{'='*55}\")\n",
    "        print(f\"Reference {i+1}: {name}\")\n",
    "        print(f\"Ref text: {ref_text[:70]}...\")\n",
    "        \n",
    "        print(\"\\nOriginal reference voice:\")\n",
    "        display(Audio(wav, rate=sr))\n",
    "        \n",
    "        # Clone the voice\n",
    "        cloned = tts.infer(\n",
    "            text=target_text,\n",
    "            ref_audio=ref_audio_path,\n",
    "            ref_text=ref_text\n",
    "        )\n",
    "        cloned_np = np.array(cloned) if not isinstance(cloned, np.ndarray) else cloned\n",
    "        cloned_audios.append(cloned_np)\n",
    "        \n",
    "        print(f\"Cloned voice (saying: '{target_text[:40]}...')\")\n",
    "        display(Audio(cloned_np, rate=24000))\n",
    "        print(f\"Generated: {len(cloned_np)/24000:.2f}s\")\n",
    "    \n",
    "    tts.close()\n",
    "    \n",
    "    # Compare spectrograms of cloned voices\n",
    "    fig, axes = plt.subplots(1, len(cloned_audios), figsize=(7 * len(cloned_audios), 5))\n",
    "    if len(cloned_audios) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (audio, name) in enumerate(zip(cloned_audios, all_names)):\n",
    "        mel = librosa.feature.melspectrogram(y=audio, sr=24000, n_mels=80,\n",
    "                                              hop_length=256, n_fft=1024)\n",
    "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
    "        librosa.display.specshow(mel_db, sr=24000, hop_length=256,\n",
    "                                  y_axis='mel', x_axis='time',\n",
    "                                  ax=axes[i], cmap='viridis')\n",
    "        axes[i].set_title(f\"Cloned: Speaker {i+1} ({name})\", fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f\"Same text, three different cloned voices\\n\\\"{target_text[:50]}...\\\"\\n\"\n",
    "                  \"Note different spectral envelopes despite identical content\",\n",
    "                  fontsize=11, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"cloned_voice_spectrograms.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"[DEMO MODE] VieNeu not installed. Showing reference audios instead.\")\n",
    "    print()\n",
    "    for i, (wav, sr, name) in enumerate(ref_audios):\n",
    "        print(f\"Reference {i+1}: {name}\")\n",
    "        print(f\"  Text: {ref_texts[i][:80] if i < len(ref_texts) else 'N/A'}\")\n",
    "        display(Audio(wav, rate=sr))\n",
    "        print()\n",
    "    \n",
    "    print(\"With VieNeu-TTS installed, running:\")\n",
    "    print(\"  tts.infer(text='...', ref_audio='example.wav', ref_text='...')\")\n",
    "    print(\"would synthesize the target text in each speaker's voice.\")\n",
    "    print()\n",
    "    print(\"Expected behavior: All three outputs say the same sentence\")\n",
    "    print(\"but sound like speakers 1, 2, and 3 respectively.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch06-similarity-header",
   "metadata": {},
   "source": [
    "## 4. Speaker Similarity — Cosine Distance\n",
    "\n",
    "We use the codec token distribution as a proxy for speaker embeddings to compute similarity. In a real system, you would use a pretrained speaker encoder (e.g., WeSpeaker, SpeechBrain).\n",
    "\n",
    "**Cosine similarity:**\n",
    "$$\\text{SECS}(e_\\text{ref}, e_\\text{gen}) = \\frac{e_\\text{ref} \\cdot e_\\text{gen}}{\\|e_\\text{ref}\\| \\cdot \\|e_\\text{gen}\\|}$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Self-similarity (speaker vs themselves): should be close to 1.0\n",
    "- Cross-similarity (speaker A vs speaker B): should be lower\n",
    "- Clone quality: similarity between reference and generated audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch06-similarity-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize(v: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"L2-normalize a vector.\"\"\"\n",
    "    return v / (np.linalg.norm(v) + 1e-8)\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"Cosine similarity between two vectors.\"\"\"\n",
    "    return float(np.dot(normalize(a), normalize(b)))\n",
    "\n",
    "def codes_to_histogram(codes: np.ndarray, n_bins: int = 256) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert codec token sequence to a normalized histogram.\n",
    "    This is a simplified speaker embedding proxy:\n",
    "    real systems use a neural speaker encoder, but the histogram\n",
    "    captures the token distribution characteristic of each voice.\n",
    "    \"\"\"\n",
    "    # Bin the token IDs (0-65535) into n_bins buckets\n",
    "    hist, _ = np.histogram(codes, bins=n_bins, range=(0, 65535))\n",
    "    return hist.astype(float)\n",
    "\n",
    "n_speakers = len(all_codes)\n",
    "speaker_names = [f\"Speaker {i+1}\\n({name})\" for i, name in enumerate(all_names)]\n",
    "\n",
    "# Compute pairwise similarity matrix\n",
    "histograms = [codes_to_histogram(codes) for codes in all_codes]\n",
    "sim_matrix = np.zeros((n_speakers, n_speakers))\n",
    "\n",
    "print(\"Pairwise Cosine Similarities (codec token histograms):\")\n",
    "print()\n",
    "header = f\"{'':>15}\" + \"\".join(f\"{f'Spk {i+1}':>12}\" for i in range(n_speakers))\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for i in range(n_speakers):\n",
    "    row = f\"{'Speaker ' + str(i+1):>15}\"\n",
    "    for j in range(n_speakers):\n",
    "        sim = cosine_similarity(histograms[i], histograms[j])\n",
    "        sim_matrix[i, j] = sim\n",
    "        row += f\"{sim:>12.4f}\"\n",
    "    print(row)\n",
    "\n",
    "print()\n",
    "print(\"Expected patterns:\")\n",
    "print(\"  Diagonal = 1.0 (self-similarity)\")\n",
    "print(\"  Off-diagonal < 1.0 (cross-speaker)\")\n",
    "print(\"  After cloning: sim(ref, generated) should be > sim(ref, other speaker)\")\n",
    "\n",
    "# Heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "im = axes[0].imshow(sim_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "axes[0].set_xticks(range(n_speakers))\n",
    "axes[0].set_yticks(range(n_speakers))\n",
    "axes[0].set_xticklabels(speaker_names, fontsize=9)\n",
    "axes[0].set_yticklabels(speaker_names, fontsize=9)\n",
    "axes[0].set_title(\"Speaker Similarity Matrix\\n(codec token histogram cosine similarity)\",\n",
    "                   fontsize=11)\n",
    "plt.colorbar(im, ax=axes[0], label='Cosine similarity')\n",
    "for i in range(n_speakers):\n",
    "    for j in range(n_speakers):\n",
    "        axes[0].text(j, i, f\"{sim_matrix[i,j]:.3f}\",\n",
    "                     ha='center', va='center', fontsize=11,\n",
    "                     color='black' if sim_matrix[i,j] > 0.5 else 'white',\n",
    "                     fontweight='bold')\n",
    "\n",
    "# Bar chart of cross-speaker vs self similarities\n",
    "self_sims = [sim_matrix[i, i] for i in range(n_speakers)]\n",
    "cross_sims_per_speaker = []\n",
    "for i in range(n_speakers):\n",
    "    others = [sim_matrix[i, j] for j in range(n_speakers) if j != i]\n",
    "    cross_sims_per_speaker.append(np.mean(others))\n",
    "\n",
    "x = np.arange(n_speakers)\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, self_sims, width, label='Self-similarity', color='#2ecc71', alpha=0.8)\n",
    "axes[1].bar(x + width/2, cross_sims_per_speaker, width,\n",
    "            label='Mean cross-similarity', color='#e74c3c', alpha=0.8)\n",
    "axes[1].set_xlabel(\"Speaker\", fontsize=10)\n",
    "axes[1].set_ylabel(\"Cosine similarity\", fontsize=10)\n",
    "axes[1].set_title(\"Self vs Cross-Speaker Similarity\", fontsize=11)\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f\"Speaker {i+1}\" for i in range(n_speakers)])\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "axes[1].axhline(0.85, color='orange', linestyle='--', linewidth=1,\n",
    "                label='Good clone threshold (0.85)')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"speaker_similarity_matrix.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: This uses codec histogram similarity as a proxy for speaker embeddings.\")\n",
    "print(\"For production evaluation, use a pretrained speaker encoder (WeSpeaker, SpeechBrain).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch06-reflength-header",
   "metadata": {},
   "source": [
    "## 5. Effect of Reference Length on Clone Quality\n",
    "\n",
    "How much reference audio do we need for good voice cloning? We trim the reference to different lengths (1s, 2s, 3s, 5s) and compare the codec token distribution.\n",
    "\n",
    "**Hypothesis:** Shorter references have higher variance and less complete voice information. The token histogram becomes noisier with fewer tokens, and the similarity to the full reference decreases.\n",
    "\n",
    "**Practical guideline:** 3 seconds is the sweet spot — enough tokens for a stable voice fingerprint, minimal context consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch06-reflength-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the first reference speaker for this analysis\n",
    "if len(ref_audios) > 0:\n",
    "    wav_full, sr_full, name_full = ref_audios[0]\n",
    "    full_duration = len(wav_full) / sr_full\n",
    "    print(f\"Reference: {name_full} ({full_duration:.2f}s)\")\n",
    "else:\n",
    "    # Create a synthetic signal if no audio files\n",
    "    sr_full = 22050\n",
    "    t = np.linspace(0, 8.0, int(8.0 * sr_full))\n",
    "    wav_full = 0.5 * np.sin(2 * np.pi * 180 * t) + 0.2 * np.random.randn(len(t))\n",
    "    wav_full = wav_full.astype(np.float32)\n",
    "    full_duration = 8.0\n",
    "    name_full = \"synthetic_signal\"\n",
    "    print(f\"Using synthetic signal ({full_duration:.2f}s)\")\n",
    "\n",
    "# Durations to test\n",
    "test_durations = [1.0, 1.5, 2.0, 3.0, 5.0]\n",
    "test_durations = [d for d in test_durations if d <= full_duration]\n",
    "\n",
    "TOKEN_RATE = 50  # DistillNeuCodec\n",
    "\n",
    "if NEUCODEC_AVAILABLE:\n",
    "    # Encode full reference\n",
    "    wav_16k = librosa.resample(wav_full, orig_sr=sr_full, target_sr=16000)\n",
    "    wav_t_full = torch.from_numpy(wav_16k).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        codes_full = codec.encode_code(wav_t_full).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Encode trimmed versions\n",
    "    trimmed_codes = []\n",
    "    for dur in test_durations:\n",
    "        samples = int(dur * 16000)\n",
    "        wav_trimmed = wav_16k[:samples]\n",
    "        wav_t = torch.from_numpy(wav_trimmed).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            c = codec.encode_code(wav_t).squeeze().cpu().numpy()\n",
    "        trimmed_codes.append(c)\n",
    "else:\n",
    "    # Simulate: trim the all_codes[0] sequence\n",
    "    if len(all_codes) > 0:\n",
    "        codes_full = all_codes[0]\n",
    "    else:\n",
    "        np.random.seed(42)\n",
    "        codes_full = np.random.randint(8000, 25000, size=int(TOKEN_RATE * full_duration))\n",
    "    \n",
    "    trimmed_codes = []\n",
    "    for dur in test_durations:\n",
    "        n_tokens = int(TOKEN_RATE * dur)\n",
    "        trimmed_codes.append(codes_full[:n_tokens])\n",
    "\n",
    "# Compare similarity between trimmed reference and full reference\n",
    "full_hist = codes_to_histogram(codes_full, n_bins=128)\n",
    "\n",
    "sims_to_full = []\n",
    "for codes_trim in trimmed_codes:\n",
    "    trim_hist = codes_to_histogram(codes_trim, n_bins=128)\n",
    "    sim = cosine_similarity(full_hist, trim_hist)\n",
    "    sims_to_full.append(sim)\n",
    "\n",
    "# Plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# Plot 1: Similarity vs duration\n",
    "axes[0].plot(test_durations, sims_to_full, 'o-', linewidth=2.5,\n",
    "             markersize=10, color='#3498db')\n",
    "axes[0].fill_between(test_durations, sims_to_full, alpha=0.2, color='#3498db')\n",
    "axes[0].axhline(0.95, color='green', linestyle='--', linewidth=1.5,\n",
    "                label='0.95 (very good)')\n",
    "axes[0].axhline(0.85, color='orange', linestyle='--', linewidth=1.5,\n",
    "                label='0.85 (acceptable)')\n",
    "axes[0].axvline(3.0, color='red', linestyle='--', linewidth=1.5,\n",
    "                label='3s (recommended)')\n",
    "axes[0].set_xlabel(\"Reference duration (seconds)\", fontsize=11)\n",
    "axes[0].set_ylabel(\"Similarity to full reference\", fontsize=11)\n",
    "axes[0].set_title(\"Voice Fingerprint Quality\\nvs Reference Length\", fontsize=11)\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0.5, 1.05)\n",
    "\n",
    "# Annotate each point\n",
    "for dur, sim in zip(test_durations, sims_to_full):\n",
    "    n_tokens = int(TOKEN_RATE * dur)\n",
    "    axes[0].annotate(f\"{n_tokens} tok\\nsim={sim:.3f}\",\n",
    "                     (dur, sim), textcoords='offset points',\n",
    "                     xytext=(0, 12), ha='center', fontsize=7)\n",
    "\n",
    "# Plot 2: Token count vs context usage\n",
    "context_pct = [int(TOKEN_RATE * d) / 2048 * 100 for d in test_durations]\n",
    "bars = axes[1].bar(test_durations, context_pct,\n",
    "                    color=['#e74c3c' if p > 10 else '#2ecc71' for p in context_pct],\n",
    "                    width=0.4, alpha=0.8, edgecolor='black')\n",
    "axes[1].set_xlabel(\"Reference duration (seconds)\", fontsize=11)\n",
    "axes[1].set_ylabel(\"% of 2048-token context used\", fontsize=11)\n",
    "axes[1].set_title(\"Context Window Cost\\nof Reference Clip\", fontsize=11)\n",
    "axes[1].axhline(15, color='orange', linestyle='--', label='15% threshold')\n",
    "for bar, pct in zip(bars, context_pct):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
    "                 f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Histogram comparison (full vs 1s vs 3s)\n",
    "compare_durs = [1.0, 3.0]\n",
    "compare_idxs = [test_durations.index(d) for d in compare_durs if d in test_durations]\n",
    "\n",
    "bins = np.linspace(0, 65535, 65)\n",
    "axes[2].hist(codes_full, bins=bins, alpha=0.4, color='black', label=f'Full ({full_duration:.1f}s)')\n",
    "plot_colors = ['#e74c3c', '#2ecc71', '#3498db']\n",
    "for k, idx in enumerate(compare_idxs):\n",
    "    dur = test_durations[idx]\n",
    "    c = trimmed_codes[idx]\n",
    "    axes[2].hist(c, bins=bins, alpha=0.5, color=plot_colors[k],\n",
    "                  label=f'{dur:.1f}s ({int(TOKEN_RATE*dur)} tokens)')\n",
    "axes[2].set_xlabel(\"Token ID\", fontsize=11)\n",
    "axes[2].set_ylabel(\"Count\", fontsize=11)\n",
    "axes[2].set_title(\"Token Distribution:\\nFull vs Trimmed Reference\", fontsize=11)\n",
    "axes[2].legend(fontsize=9)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Effect of Reference Length on Voice Fingerprint Quality\",\n",
    "             fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reference_length_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary: Reference length trade-offs\")\n",
    "print(f\"{'Duration':>10} {'Tokens':>8} {'Context%':>10} {'Similarity':>12}\")\n",
    "print(\"-\" * 45)\n",
    "for dur, sim, ctx in zip(test_durations, sims_to_full, context_pct):\n",
    "    n = int(TOKEN_RATE * dur)\n",
    "    verdict = \"[RECOMMENDED]\" if dur == 3.0 else \"\"\n",
    "    print(f\"{dur:>10.1f}s {n:>8} {ctx:>9.1f}% {sim:>12.4f}  {verdict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch06-codeswitch-header",
   "metadata": {},
   "source": [
    "## 6. Code-Switching Vietnamese + English\n",
    "\n",
    "Vietnamese technical speech naturally mixes Vietnamese and English. VieNeu-TTS handles this because:\n",
    "1. The tokenizer includes both Vietnamese and English subwords\n",
    "2. Training data contains code-switched speech\n",
    "3. `espeak-ng` phonemizes English words with proper IPA\n",
    "\n",
    "**Examples of common Vietnamese-English code-switching:**\n",
    "- Technical terms: \"machine learning\", \"GPU\", \"batch size\"\n",
    "- Brand names: \"Google\", \"Apple\", \"ChatGPT\"\n",
    "- Programming: \"Python\", \"PyTorch\", \"model\"\n",
    "\n",
    "Notice how the model must switch phonetic system (Vietnamese tonal vs English stress-based) within a single utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch06-codeswitch-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mixed_texts = [\n",
    "    # Vietnamese with English technical terms\n",
    "    \"Machine learning đang thay đổi ngành công nghiệp.\",\n",
    "    \"Model được train trên GPU với batch size là 32.\",\n",
    "    \"Chúng ta sử dụng Python và PyTorch để xây dựng AI.\",\n",
    "    \"ChatGPT và Claude là những mô hình ngôn ngữ lớn nổi tiếng.\",\n",
    "]\n",
    "\n",
    "print(\"Code-switching examples (Vietnamese + English):\")\n",
    "print()\n",
    "for i, text in enumerate(mixed_texts):\n",
    "    # Identify English words (simple heuristic: ASCII only)\n",
    "    words = text.split()\n",
    "    annotated = []\n",
    "    for word in words:\n",
    "        clean = word.strip('.,?!')\n",
    "        is_english = all(ord(c) < 128 for c in clean) and clean.isalpha()\n",
    "        if is_english and len(clean) > 1:\n",
    "            annotated.append(f\"[EN:{word}]\")\n",
    "        else:\n",
    "            annotated.append(word)\n",
    "    print(f\"  {i+1}. {' '.join(annotated)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "if VIENEU_AVAILABLE:\n",
    "    tts = Vieneu()\n",
    "    \n",
    "    # Use first available reference voice\n",
    "    ref_audio_path = REF_FILES[0]\n",
    "    ref_text = ref_texts[0] if ref_texts else \"Xin chào đây là câu mẫu.\"\n",
    "    \n",
    "    print(f\"Using reference voice: {ref_audio_path.split('/')[-1]}\")\n",
    "    print()\n",
    "    \n",
    "    generated_audios = []\n",
    "    for text in mixed_texts:\n",
    "        print(f\"Text: {text}\")\n",
    "        audio = tts.infer(text=text, ref_audio=ref_audio_path, ref_text=ref_text)\n",
    "        audio_np = np.array(audio) if not isinstance(audio, np.ndarray) else audio\n",
    "        generated_audios.append(audio_np)\n",
    "        display(Audio(audio_np, rate=24000))\n",
    "        print(f\"  Duration: {len(audio_np)/24000:.2f}s\")\n",
    "        print()\n",
    "    \n",
    "    tts.close()\n",
    "    \n",
    "    # Visualize pitch contours (Vietnamese tones should be visible in F0)\n",
    "    import librosa\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "    for ax, audio, text in zip(axes.flatten(), generated_audios, mixed_texts):\n",
    "        f0, voiced_flag, _ = librosa.pyin(\n",
    "            audio, fmin=librosa.note_to_hz('C2'),\n",
    "            fmax=librosa.note_to_hz('C7'), sr=24000, hop_length=256\n",
    "        )\n",
    "        times = librosa.times_like(f0, sr=24000, hop_length=256)\n",
    "        ax.plot(times[voiced_flag], f0[voiced_flag], '.', markersize=3, color='#2ecc71')\n",
    "        ax.set_title(f\"{text[:50]}...\", fontsize=8, wrap=True)\n",
    "        ax.set_xlabel(\"Time (s)\", fontsize=8)\n",
    "        ax.set_ylabel(\"F0 (Hz)\", fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(\"Pitch Contours of Code-Switched Utterances\\n\"\n",
    "                  \"Vietnamese tonal patterns + English stress patterns visible\",\n",
    "                  fontsize=11, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"codeswitching_pitch.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"[DEMO MODE] Showing expected behavior for code-switching:\")\n",
    "    print()\n",
    "    print(\"Phonemization breakdown (what the model receives):\")\n",
    "    print()\n",
    "    \n",
    "    # Show what the phonemization would look like\n",
    "    phoneme_examples = [\n",
    "        (\"Machine learning đang thay đổi ngành công nghiệp.\",\n",
    "         \"[məˈʃiːn] [ˈlɜːnɪŋ] /ðaŋ/ /tʰaj/ /ðôj/ /ŋàŋ/ /koŋ/ /ŋjəp/\"),\n",
    "        (\"Model được train trên GPU với batch size là 32.\",\n",
    "         \"[ˈmɒdəl] /ðɨɤk/ [tɹeɪn] /tɕɛn/ [dʒiːpiːjuː] /vɤj/ [bætʃ] [saɪz] /là/ ba mươi hai\"),\n",
    "    ]\n",
    "    for text, phones in phoneme_examples:\n",
    "        print(f\"  Input:  {text}\")\n",
    "        print(f\"  Phones: {phones}\")\n",
    "        print(f\"          └─ [] = English IPA, /\\ = Vietnamese phonemes\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch06-limitations-header",
   "metadata": {},
   "source": [
    "## 7. Limitations Summary Table\n",
    "\n",
    "Understanding failure modes is essential for building reliable Vietnamese TTS applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch06-limitations-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# Limitations and their properties\n",
    "limitations = [\n",
    "    # (name, severity, frequency, mitigation_exists)\n",
    "    (\"Accent transfer\",          0.6, 0.7, True,\n",
    "     \"Voice accent bleeds into output.\\nMitigation: Use neutral reference clip.\"),\n",
    "    (\"Emotional mismatch\",       0.5, 0.6, False,\n",
    "     \"Neutral ref → flat emotional output.\\nMitigation: Use emotionally appropriate ref.\"),\n",
    "    (\"Out-of-domain text\",       0.7, 0.4, True,\n",
    "     \"Unusual phonemes → voice inconsistency.\\nMitigation: Normalize text preprocessing.\"),\n",
    "    (\"Voice drift (long output)\", 0.65, 0.3, True,\n",
    "     \"Voice drifts after ~15s generation.\\nMitigation: Segment into <10s chunks.\"),\n",
    "    (\"Dialect mismatch\",         0.75, 0.45, True,\n",
    "     \"N/S/Central dialect conflicts.\\nMitigation: Match dialect in ref+text.\"),\n",
    "    (\"Noisy reference\",          0.85, 0.5, True,\n",
    "     \"SNR < 20dB → noise encoded.\\nMitigation: Use clean audio only.\"),\n",
    "    (\"Short reference (<1.5s)\",  0.8, 0.35, True,\n",
    "     \"Insufficient voice info.\\nMitigation: Use ≥ 3s reference.\"),\n",
    "    (\"Tone errors (high τ)\",     0.9, 0.25, True,\n",
    "     \"Wrong tone = wrong word (Vietnamese).\\nMitigation: Keep temperature ≤ 1.0.\"),\n",
    "]\n",
    "\n",
    "# Severity vs Frequency scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "colors_scatter = ['#e74c3c' if not m else '#2ecc71' for _, _, _, m, _ in limitations]\n",
    "sizes = [sev * 500 for _, sev, _, _, _ in limitations]\n",
    "\n",
    "ax = axes[0]\n",
    "for i, (name, sev, freq, mit, desc) in enumerate(limitations):\n",
    "    color = '#e74c3c' if not mit else '#2ecc71'\n",
    "    ax.scatter(freq, sev, s=sizes[i], alpha=0.6, color=color, edgecolors='black', linewidth=0.5)\n",
    "    ax.annotate(name, (freq, sev), textcoords='offset points',\n",
    "                xytext=(5, 5), fontsize=8)\n",
    "\n",
    "ax.set_xlabel(\"Frequency of occurrence\", fontsize=11)\n",
    "ax.set_ylabel(\"Impact severity\", fontsize=11)\n",
    "ax.set_title(\"Voice Cloning Limitations\\n(bubble size = severity)\", fontsize=11)\n",
    "ax.set_xlim(0, 1); ax.set_ylim(0, 1)\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.axvline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.text(0.75, 0.9, \"High impact,\\nfrequent\", ha='center', fontsize=8, color='red',\n",
    "        bbox=dict(boxstyle='round', facecolor='#ffeaa7', alpha=0.8))\n",
    "ax.text(0.15, 0.15, \"Low impact,\\nrare\", ha='center', fontsize=8, color='green',\n",
    "        bbox=dict(boxstyle='round', facecolor='#d5f5e3', alpha=0.8))\n",
    "\n",
    "patches = [\n",
    "    mpatches.Patch(color='#2ecc71', alpha=0.6, label='Mitigation available'),\n",
    "    mpatches.Patch(color='#e74c3c', alpha=0.6, label='Hard to mitigate'),\n",
    "]\n",
    "ax.legend(handles=patches, fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Checklist table\n",
    "ax2 = axes[1]\n",
    "ax2.axis('off')\n",
    "\n",
    "checklist_rows = [\n",
    "    (\"Pre-synthesis checklist for Vietnamese voice cloning:\", \"\", \"\"),\n",
    "    (\"\", \"\", \"\"),\n",
    "    (\"REFERENCE AUDIO\", \"\", \"\"),\n",
    "    (\"  Duration\",         \"≥ 3 seconds\",          \"Covers phoneme diversity\"),\n",
    "    (\"  SNR\",              \"> 25 dB\",               \"Avoid encoding noise\"),\n",
    "    (\"  Tones\",            \"Multiple tone classes\", \"Captures tonal range\"),\n",
    "    (\"  Dialect\",          \"Matches target\",        \"N/S/Central consistency\"),\n",
    "    (\"  Emotion\",          \"Neutral (for general)\", \"Avoids emotion mismatch\"),\n",
    "    (\"\", \"\", \"\"),\n",
    "    (\"TEXT PREPROCESSING\", \"\", \"\"),\n",
    "    (\"  Abbreviations\",    \"Expand: AI → A. I.\",   \"Better phonemization\"),\n",
    "    (\"  Numbers\",          \"Verbalize: 32 → ba...\", \"Correct pronunciation\"),\n",
    "    (\"  Segment length\",   \"< 10 seconds each\",     \"Prevent voice drift\"),\n",
    "    (\"\", \"\", \"\"),\n",
    "    (\"MODEL PARAMETERS\", \"\", \"\"),\n",
    "    (\"  Temperature\",      \"τ = 1.0 (default)\",     \"Safe for tones\"),\n",
    "    (\"  Top-k\",            \"k = 50 (default)\",      \"Prevents glitches\"),\n",
    "]\n",
    "\n",
    "y = 0.98\n",
    "for row in checklist_rows:\n",
    "    label, value, note = row\n",
    "    if label.isupper() and value == \"\":\n",
    "        # Section header\n",
    "        ax2.text(0.02, y, label, transform=ax2.transAxes,\n",
    "                 fontsize=9, fontweight='bold', color='#2c3e50',\n",
    "                 verticalalignment='top')\n",
    "    elif label:\n",
    "        text = f\"{label:20s} {value:25s}  # {note}\"\n",
    "        ax2.text(0.02, y, text, transform=ax2.transAxes,\n",
    "                 fontsize=8, fontfamily='monospace', color='#2c3e50',\n",
    "                 verticalalignment='top')\n",
    "    y -= 0.055\n",
    "\n",
    "ax2.set_title(\"Production Checklist\", fontsize=11)\n",
    "ax2.set_facecolor('#f9f9f9')\n",
    "\n",
    "plt.suptitle(\"Zero-Shot Voice Cloning: Limitations and Mitigations\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cloning_limitations.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch06-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Zero-shot voice cloning = in-context learning for audio.**\n",
    "\n",
    "Just as a text LLM can follow instructions from a prompt without fine-tuning, VieNeu-TTS clones voices from reference tokens in the prompt without fine-tuning.\n",
    "\n",
    "**Core mechanism:**\n",
    "$$P_\\theta(s_t \\mid s_{<t}, \\underbrace{\\text{ref\\_codes}}_{\\text{voice fingerprint}}, \\mathbf{x}) \\xrightarrow{\\text{autoregressive}} \\text{cloned speech}$$\n",
    "\n",
    "**Key parameters for Vietnamese:**\n",
    "\n",
    "| Parameter | Value | Reason |\n",
    "|---|---|---|\n",
    "| Reference duration | 3-5s | Sufficient tone + phoneme coverage |\n",
    "| Reference SNR | > 25 dB | Clean token encoding |\n",
    "| Temperature | τ = 1.0 | Tone accuracy vs prosody balance |\n",
    "| Segment length | < 10s | Prevent voice drift |\n",
    "| Dialect | Match ref ↔ text | Phonetic consistency |\n",
    "\n",
    "**Next chapter:** Fine-tuning VieNeu-TTS on custom Vietnamese voices — when zero-shot is not enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
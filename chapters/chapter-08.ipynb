{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ch08-cell-01",
   "metadata": {},
   "source": ["# Chapter 8 — Data Preparation & Quality"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch08-cell-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, librosa, soundfile as sf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch08-cell-03",
   "metadata": {},
   "source": ["## 1. Load and Analyze the Example Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch08-cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = \"../examples/audio_ref\"\n",
    "files = sorted([f for f in os.listdir(audio_dir) if f.endswith(\".wav\")])\n",
    "\n",
    "stats = []\n",
    "for f in files:\n",
    "    path = os.path.join(audio_dir, f)\n",
    "    info = sf.info(path)\n",
    "    wav, sr = librosa.load(path, sr=None)\n",
    "    rms_db = 20 * np.log10(np.sqrt(np.mean(wav**2)) + 1e-9)\n",
    "    intervals = librosa.effects.split(wav, top_db=25)\n",
    "    speech_samples = sum(e - s for s, e in intervals)\n",
    "    stats.append({\n",
    "        \"file\": f,\n",
    "        \"duration_s\": round(info.duration, 2),\n",
    "        \"sample_rate\": info.samplerate,\n",
    "        \"rms_db\": round(rms_db, 1),\n",
    "        \"speech_ratio\": round(speech_samples / len(wav), 2)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(stats)\n",
    "print(df.to_string())\n",
    "print(f\"\\nMean duration: {df['duration_s'].mean():.2f}s\")\n",
    "print(f\"All within 3-15s: {((df['duration_s'] >= 3) & (df['duration_s'] <= 15)).all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch08-cell-05",
   "metadata": {},
   "source": ["## 2. SNR Estimation"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch08-cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_snr(wav, top_db=30):\n",
    "    intervals = librosa.effects.split(wav, top_db=top_db)\n",
    "    if len(intervals) == 0:\n",
    "        return 0.0\n",
    "    speech = np.concatenate([wav[s:e] for s, e in intervals])\n",
    "    speech_power = np.mean(speech**2)\n",
    "    mask = np.zeros(len(wav), dtype=bool)\n",
    "    for s, e in intervals:\n",
    "        mask[s:e] = True\n",
    "    silence = wav[~mask]\n",
    "    if len(silence) < 100:\n",
    "        return 60.0\n",
    "    noise_power = np.mean(silence**2)\n",
    "    if noise_power < 1e-10:\n",
    "        return 60.0\n",
    "    return 10 * np.log10(speech_power / (noise_power + 1e-10))\n",
    "\n",
    "for f in files:\n",
    "    path = os.path.join(audio_dir, f)\n",
    "    wav, sr = librosa.load(path, sr=None)\n",
    "    snr = estimate_snr(wav)\n",
    "    status = \"Good (>25dB)\" if snr > 25 else \"Check\"\n",
    "    print(f\"{f}: SNR = {snr:.1f} dB  [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch08-cell-07",
   "metadata": {},
   "source": ["## 3. Vietnamese Tone Distribution"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch08-cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = sorted([f for f in os.listdir(audio_dir) if f.endswith(\".txt\")])\n",
    "all_text = \"\"\n",
    "for f in txt_files:\n",
    "    with open(os.path.join(audio_dir, f), encoding=\"utf-8\") as fh:\n",
    "        all_text += fh.read() + \" \"\n",
    "\n",
    "tone_patterns = {\n",
    "    \"huyen\": r\"[\\u00e0\\u1eb1\\u1ea7\\u1ea9\\u1eab\\u1ead\\u00e8\\u1ec1\\u1ec3\\u1ec5\\u1ec7\\u00ec\\u1ec9\\u1ecb\\u00f2\\u1ed3\\u1ed5\\u1ed7\\u1ed9\\u00f9\\u1eeb\\u1eed\\u1eef\\u1ef1\\u1ef3\\u1ef7\\u1ef9\\u1ef5]\",\n",
    "    \"sac\":   r\"[\\u00e1\\u1eaf\\u1ea5\\u00e9\\u1ebf\\u00ed\\u00f3\\u1ed1\\u00fa\\u1ee9\\u00fd]\",\n",
    "    \"hoi\":   r\"[\\u1ea3\\u1eb3\\u1ea9\\u1ebb\\u1ec3\\u1ec9\\u1ecf\\u1ed5\\u1ee7\\u1eed\\u1ef7]\",\n",
    "    \"nga\":   r\"[\\u00e3\\u1eb5\\u1eab\\u1ebd\\u1ec5\\u0129\\u00f5\\u1ed7\\u0169\\u1ef1\\u1ef9]\",\n",
    "    \"nang\":  r\"[\\u1ea1\\u1eb7\\u1ead\\u1eb9\\u1ec7\\u1ecb\\u1ecd\\u1ed9\\u1ee5\\u1ef1\\u1ef5]\",\n",
    "}\n",
    "counts = {}\n",
    "for tone, pat in tone_patterns.items():\n",
    "    counts[tone] = len(re.findall(pat, all_text))\n",
    "total = len(re.findall(r\"[aă\\u00e2e\\u00eaioô\\u01a1u\\u01b0y]\", all_text, re.IGNORECASE))\n",
    "counts[\"ngang\"] = max(0, total - sum(counts.values()))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "colors = [\"#2196F3\",\"#4CAF50\",\"#FF9800\",\"#F44336\",\"#9C27B0\",\"#795548\"]\n",
    "bars = plt.bar(list(counts.keys()), list(counts.values()), color=colors)\n",
    "plt.title(\"Vietnamese Tone Distribution in Example Dataset\")\n",
    "plt.ylabel(\"Vowel count\")\n",
    "plt.xlabel(\"Tone\")\n",
    "for bar, count in zip(bars, counts.values()):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             str(count), ha=\"center\", va=\"bottom\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch08-cell-09",
   "metadata": {},
   "source": ["## 4. Filter Pipeline — What Gets Removed and Why"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch08-cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_filter(text):\n",
    "    \"\"\"Mirrors finetune/data_scripts/filter_data.py logic\"\"\"\n",
    "    if not text:\n",
    "        return False, \"empty text\"\n",
    "    if re.search(r\"\\d\", text):\n",
    "        return False, \"contains digits\"\n",
    "    if re.compile(r\"(?:[a-zA-Z]\\.){2,}\").search(text):\n",
    "        return False, \"contains acronym (with dots)\"\n",
    "    if re.compile(r\"(?:[A-Z]){2,}\").search(text):\n",
    "        return False, \"contains acronym (uppercase)\"\n",
    "    if text.strip()[-1] not in \".,?!\":\n",
    "        return False, \"missing end punctuation\"\n",
    "    return True, \"OK\"\n",
    "\n",
    "test_cases = [\n",
    "    \"Xin ch\\u00e0o Vi\\u1ec7t Nam.\",\n",
    "    \"N\\u0103m 2024 l\\u00e0 n\\u0103m quan tr\\u1ecdng.\",\n",
    "    \"AI \\u0111ang ph\\u00e1t tri\\u1ec3n m\\u1ea1nh.\",\n",
    "    \"H\\u1ec7 th\\u1ed1ng TTS ho\\u1ea1t \\u0111\\u1ed9ng t\\u1ed1t.\",\n",
    "    \"T\\u00f4i th\\u00edch h\\u1ecdc machine learning\",\n",
    "    \"H\\u00f4m nay tr\\u1eddi \\u0111\\u1eb9p l\\u1eafm.\",\n",
    "]\n",
    "\n",
    "for text in test_cases:\n",
    "    passed, reason = text_filter(text)\n",
    "    icon = \"PASS\" if passed else \"FAIL\"\n",
    "    print(f\"[{icon}] {text!r:45s}  reason: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch08-cell-11",
   "metadata": {},
   "source": ["## 5. Audio Preprocessing Pipeline"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch08-cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "wav, sr = librosa.load(\"../examples/audio_ref/example.wav\", sr=16000)\n",
    "\n",
    "# Step 1: Peak normalization\n",
    "wav_norm = wav / (np.max(np.abs(wav)) + 1e-8) * 0.95\n",
    "\n",
    "# Step 2: Silence trimming\n",
    "wav_trimmed, _ = librosa.effects.trim(wav_norm, top_db=25)\n",
    "\n",
    "# Step 3: DC offset removal\n",
    "wav_clean = wav_trimmed - np.mean(wav_trimmed)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 7))\n",
    "for ax, w, title in zip(axes,\n",
    "    [wav, wav_norm, wav_clean],\n",
    "    [\"Original\", \"Peak normalized (0.95)\", \"Trimmed + DC removed\"]):\n",
    "    t = np.linspace(0, len(w)/16000, len(w))\n",
    "    ax.plot(t, w, alpha=0.7, linewidth=0.5)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Amplitude\")\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original:     {len(wav)/16000:.3f}s\")\n",
    "print(f\"After trim:   {len(wav_clean)/16000:.3f}s\")\n",
    "print(f\"Removed:      {(len(wav)-len(wav_clean))/16000:.3f}s of silence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch08-cell-13",
   "metadata": {},
   "source": [
    "## 6. Dataset Quality Checklist\n",
    "\n",
    "Before running the training pipeline, verify:\n",
    "\n",
    "| Check | Requirement | Why |\n",
    "|-------|-------------|-----|\n",
    "| Duration | 3–15 s per clip | Filter requirement |\n",
    "| Sample rate | ≥ 16 kHz | Codec encode requirement |\n",
    "| SNR | > 25 dB | Clean training signal |\n",
    "| Tone coverage | All 6 tones present | Avoid tone bias |\n",
    "| No digits | Text only | Ambiguous pronunciation |\n",
    "| No acronyms | Written out | Ambiguous pronunciation |\n",
    "| Punctuation | Ends with `.`, `,`, `?`, `!` | Filter requirement |\n",
    "| Single speaker | Yes | Voice consistency |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

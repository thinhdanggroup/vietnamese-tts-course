{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ch05-title",
   "metadata": {},
   "source": [
    "# Chapter 5 — LLM-Based TTS: The VieNeu-TTS Architecture\n",
    "\n",
    "**Goal:** See exactly how VieNeu-TTS turns speech synthesis into next-token prediction — building the prompt, running inference, and understanding each component.\n",
    "\n",
    "**What you will do:**\n",
    "1. Encode a Vietnamese audio reference → speech token sequence\n",
    "2. Construct the full VieNeu-TTS prompt from scratch\n",
    "3. Visualize the causal attention mask\n",
    "4. Experiment with temperature sampling and see effects on Vietnamese tones\n",
    "5. Run full inference with VieNeu-TTS and measure RTF\n",
    "6. Understand the KV-cache speedup\n",
    "\n",
    "**Prerequisites:** Chapter 4 (codecs and token rates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch05-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n# Add project root to path\nsys.path.insert(0, \"..\")\n\nprint(f\"Python: {sys.version}\")\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\n# Check optional dependencies\ntry:\n    from neucodec import DistillNeuCodec\n    NEUCODEC_AVAILABLE = True\n    print(\"neucodec: available\")\nexcept ImportError:\n    NEUCODEC_AVAILABLE = False\n    print(\"neucodec: NOT installed\")\n\ntry:\n    from vieneu import Vieneu\n    VIENEU_AVAILABLE = True\n    print(\"vieneu: available\")\nexcept ImportError:\n    VIENEU_AVAILABLE = False\n    print(\"vieneu: NOT installed (run from VieNeu-TTS root with correct env)\")\n\ntry:\n    from vieneu_utils.phonemize_text import phonemize_with_dict\n    PHONEMIZE_AVAILABLE = True\n    print(\"vieneu_utils: available\")\nexcept ImportError:\n    PHONEMIZE_AVAILABLE = False\n    print(\"vieneu_utils: not found\")\n\n# ── Path resolver (works regardless of Jupyter CWD) ──────────────\nfrom pathlib import Path\nimport os as _os\n\ndef _find_examples_dir():\n    # Walk up from CWD (works locally when kernel starts in chapters/)\n    for _p in [Path(_os.getcwd())] + list(Path(_os.getcwd()).parents):\n        _d = _p / \"examples\" / \"audio_ref\"\n        if _d.is_dir():\n            return _d\n    # Colab fallback paths\n    for _candidate in [\n        Path(\"/content/vietnamese-tts-course/examples/audio_ref\"),\n        Path(\"/content/VieNeu-TTS/examples/audio_ref\"),\n    ]:\n        if _candidate.is_dir():\n            return _candidate\n    raise FileNotFoundError(\n        \"examples/audio_ref/ not found. \"\n        \"Clone the repo: git clone https://github.com/thinhdanggroup/vietnamese-tts-course.git\"\n    )\n\nEXAMPLES_DIR = _find_examples_dir()\nprint(f\"Examples dir: {EXAMPLES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch05-tokens-header",
   "metadata": {},
   "source": [
    "## 1. Speech as a Token Sequence\n",
    "\n",
    "The first step of VieNeu-TTS inference is encoding the reference audio into discrete tokens. These tokens will become part of the prompt — they tell the model **what voice to use**.\n",
    "\n",
    "**Math:** codec encoder → $z_e$ → VQ → integer sequence $[s_1, s_2, \\ldots, s_T]$\n",
    "\n",
    "Each $s_t \\in \\{0, 1, \\ldots, 65535\\}$ — one of 65,536 possible acoustic units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch05-tokens-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport librosa\nimport numpy as np\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nref_path = str(EXAMPLES_DIR / \"example.wav\")\nref_text_path = str(EXAMPLES_DIR / \"example.txt\")\n\n# Load reference text\ntry:\n    ref_text_content = open(ref_text_path).read().strip()\n    print(f\"Reference text: {ref_text_content}\")\nexcept FileNotFoundError:\n    ref_text_content = \"Xin chào, đây là câu ví dụ tiếng Việt.\"\n    print(f\"[Using default] Reference text: {ref_text_content}\")\n\n# Load reference audio\nwav, sr = librosa.load(ref_path, sr=16000, mono=True)\nduration_s = wav.shape[0] / sr\nprint(f\"\\nReference audio: {duration_s:.2f}s at {sr} Hz\")\n\nif NEUCODEC_AVAILABLE:\n    codec = DistillNeuCodec.from_pretrained(\"neuphonic/distill-neucodec\").to(device).eval()\n    wav_t = torch.from_numpy(wav).float().unsqueeze(0).unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        ref_codes_tensor = codec.encode_code(wav_t)\n    \n    ref_codes = ref_codes_tensor.squeeze().cpu().tolist()\n    token_rate = len(ref_codes) / duration_s\n    \n    print(f\"\\nCodec tokens:\")\n    print(f\"  Total tokens: {len(ref_codes)}\")\n    print(f\"  Token rate:   {token_rate:.1f} tokens/sec\")\n    print(f\"  First 10 tokens: {ref_codes[:10]}\")\n    print(f\"  Token range: [{min(ref_codes)}, {max(ref_codes)}]\")\n    print(f\"  Unique values: {len(set(ref_codes))}\")\nelse:\n    # Simulate plausible ref_codes for demonstration\n    np.random.seed(42)\n    n_tokens = int(50 * duration_s)\n    ref_codes = np.random.randint(0, 65536, size=n_tokens).tolist()\n    token_rate = 50.0\n    print(f\"\\n[DEMO MODE] Simulated codec tokens:\")\n    print(f\"  Total tokens: {len(ref_codes)}\")\n    print(f\"  Token rate:   {token_rate:.1f} tokens/sec\")\n    print(f\"  First 10 tokens: {ref_codes[:10]}\")\n\nprint(f\"\\nThese {len(ref_codes)} integers encode {duration_s:.1f}s of audio\")\nprint(f\"This is the 'voice fingerprint' that will be inserted into the LLM prompt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch05-prompt-header",
   "metadata": {},
   "source": [
    "## 2. Building the VieNeu-TTS Prompt\n",
    "\n",
    "The full prompt structure is:\n",
    "\n",
    "```\n",
    "user: Convert the text to speech:\n",
    "<|TEXT_PROMPT_START|>{ref_phonemes} {input_phonemes}<|TEXT_PROMPT_END|>\n",
    "assistant:<|SPEECH_GENERATION_START|>{ref_codes}\n",
    "```\n",
    "\n",
    "The model then autoregressively generates the rest — the speech tokens for the input text.\n",
    "\n",
    "**Key design choices:**\n",
    "- `ref_phonemes` and `input_phonemes` are **concatenated** (joint phonemization)\n",
    "- `ref_codes` are the actual codec integers from the reference audio\n",
    "- The model never sees the target speech tokens during inference — it generates them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch05-prompt-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "input_text = \"Xin chào, đây là mô hình tổng hợp tiếng Việt.\"\n",
    "\n",
    "# Phonemize (use real phonemizer if available, otherwise show placeholder)\n",
    "if PHONEMIZE_AVAILABLE:\n",
    "    ref_phones = phonemize_with_dict(ref_text_content)\n",
    "    input_phones = phonemize_with_dict(input_text)\n",
    "else:\n",
    "    # Placeholder showing what phonemization looks like\n",
    "    ref_phones = \"ɕɪn̟ tɕàw ðâj là kâw vɪ dụ tjɛ̌ŋ vjɛt\"\n",
    "    input_phones = \"ɕɪn̟ tɕàw ðâj là moː hɪ̀n̟ tɔ̂ŋ hɤ̂p tjɛ̌ŋ vjɛt\"\n",
    "    print(\"[DEMO MODE] Using placeholder phonemes (install espeak-ng for real phonemization)\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHONEMIZATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Reference text:\")\n",
    "print(f\"  Raw:    {ref_text_content}\")\n",
    "print(f\"  Phones: {ref_phones[:100]}{'...' if len(ref_phones) > 100 else ''}\")\n",
    "print(f\"\\nInput text:\")\n",
    "print(f\"  Raw:    {input_text}\")\n",
    "print(f\"  Phones: {input_phones}\")\n",
    "\n",
    "# Build the prompt\n",
    "codes_str = \"\".join([f\"<|speech_{idx}|>\" for idx in ref_codes])\n",
    "\n",
    "# Show a readable version of the prompt structure\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROMPT STRUCTURE\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"[ROLE] user: Convert the text to speech:\")\n",
    "print()\n",
    "print(\"[TEXT REGION] <|TEXT_PROMPT_START|>\")\n",
    "print(f\"  Ref phonemes:   {ref_phones[:60]}...\")\n",
    "print(f\"  (space)\")\n",
    "print(f\"  Input phonemes: {input_phones}\")\n",
    "print(\"<|TEXT_PROMPT_END|>\")\n",
    "print()\n",
    "print(\"[ROLE] assistant: <|SPEECH_GENERATION_START|>\")\n",
    "print(f\"  Ref codes: <|speech_{ref_codes[0]}|><|speech_{ref_codes[1]}|><|speech_{ref_codes[2]}|>...\")\n",
    "print(f\"  ({len(ref_codes)} codec tokens = {len(ref_codes)/token_rate:.1f}s of reference audio)\")\n",
    "print()\n",
    "print(\"[MODEL GENERATES HERE] ←←← autoregressive from this point\")\n",
    "print()\n",
    "\n",
    "# Token count breakdown\n",
    "approx_text_tokens = len((ref_phones + ' ' + input_phones).split())\n",
    "total_prompt_tokens = 20 + approx_text_tokens + len(ref_codes) + 4  # rough estimate\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PROMPT TOKEN COUNT (approximate)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  System text:       ~20 tokens\")\n",
    "print(f\"  Phoneme tokens:    ~{approx_text_tokens} tokens\")\n",
    "print(f\"  Special tokens:    ~4 tokens\")\n",
    "print(f\"  Reference speech:  {len(ref_codes)} tokens\")\n",
    "print(f\"  ─────────────────────────────\")\n",
    "print(f\"  Total prompt:      ~{total_prompt_tokens} tokens\")\n",
    "print(f\"  Context remaining: ~{2048 - total_prompt_tokens} tokens for generation\")\n",
    "print(f\"  Max gen duration:  ~{(2048 - total_prompt_tokens) / token_rate:.0f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch05-attn-header",
   "metadata": {},
   "source": [
    "## 3. Causal Attention Mask — Why Generation Is Autoregressive\n",
    "\n",
    "The causal attention mask enforces that position $t$ can only attend to positions $\\leq t$. This is what makes the model autoregressive: it can only use past tokens when predicting the next one.\n",
    "\n",
    "**Math:**\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right) V$$\n",
    "\n",
    "where $M_{ij} = 0$ if $j \\leq i$, else $-\\infty$.\n",
    "\n",
    "The mask below shows a 12-token sequence with tokens spanning text, special, and speech regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch05-attn-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Simulate a short sequence covering all token types in VieNeu-TTS\n",
    "seq_len = 14\n",
    "\n",
    "# Causal mask: lower triangular (1 = can attend, 0 = cannot)\n",
    "mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "\n",
    "# Token labels for our example sequence\n",
    "token_labels = [\n",
    "    \"TEXT_START\",    # 0: special token\n",
    "    \"ɕɪn̟\",         # 1: ref phoneme\n",
    "    \"tɕàw\",         # 2: ref phoneme\n",
    "    \"xɪn̟\",         # 3: input phoneme\n",
    "    \"tɕàw\",         # 4: input phoneme\n",
    "    \"TEXT_END\",      # 5: special token\n",
    "    \"SPK_START\",     # 6: special token\n",
    "    \"s_234\",         # 7: ref speech token\n",
    "    \"s_891\",         # 8: ref speech token\n",
    "    \"s_45\",          # 9: ref speech token\n",
    "    \"→GEN\",         # 10: first generated speech token\n",
    "    \"→GEN\",         # 11: second generated speech token\n",
    "    \"→GEN\",         # 12: third generated speech token\n",
    "    \"SPK_END\",       # 13: stop token\n",
    "]\n",
    "\n",
    "# Color regions for clarity\n",
    "region_colors = {\n",
    "    'text': '#AED6F1',\n",
    "    'special': '#F9E79F',\n",
    "    'ref_speech': '#A9DFBF',\n",
    "    'gen_speech': '#F1948A',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: attention mask heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(mask, cmap='Blues', vmin=0, vmax=1, aspect='equal')\n",
    "ax.set_title(\"Causal Attention Mask\\n(blue = can attend; white = blocked)\", fontsize=11)\n",
    "ax.set_xlabel(\"Key position (information source)\", fontsize=10)\n",
    "ax.set_ylabel(\"Query position (current token)\", fontsize=10)\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(token_labels, rotation=55, ha='right', fontsize=7)\n",
    "ax.set_yticklabels(token_labels, fontsize=7)\n",
    "\n",
    "# Add diagonal annotation\n",
    "for i in range(seq_len):\n",
    "    ax.add_patch(patches.Rectangle((i - 0.5, i - 0.5), 1, 1,\n",
    "                                    fill=False, edgecolor='orange', linewidth=1.5))\n",
    "\n",
    "# Draw region boundaries\n",
    "for boundary in [5.5, 6.5, 9.5, 12.5]:\n",
    "    ax.axhline(boundary, color='gray', linestyle='--', linewidth=0.8, alpha=0.6)\n",
    "    ax.axvline(boundary, color='gray', linestyle='--', linewidth=0.8, alpha=0.6)\n",
    "\n",
    "plt.colorbar(im, ax=ax, fraction=0.046, label='Attention weight (1=enabled)')\n",
    "\n",
    "# Right: explanation of what each region \"sees\"\n",
    "ax2 = axes[1]\n",
    "ax2.axis('off')\n",
    "\n",
    "explanation = [\n",
    "    (\"Token type\", \"Can attend to\", \"Notes\"),\n",
    "    (\"-\" * 20, \"-\" * 30, \"-\" * 25),\n",
    "    (\"Text tokens\", \"Past text only\", \"No speech context yet\"),\n",
    "    (\"Ref speech tokens\", \"All text + past ref\", \"Voice fingerprint established\"),\n",
    "    (\"Generated speech\", \"Text + ALL ref + past gen\", \"Voice-conditioned generation\"),\n",
    "    (\"SPEECH_END\", \"Everything\", \"Stop condition\"),\n",
    "    (\"\", \"\", \"\"),\n",
    "    (\"Key insight:\", \"\", \"\"),\n",
    "    (\"When generating s_t:\", \"\", \"\"),\n",
    "    (\"  • Sees full text\", \"\", \"→ what to say\"),\n",
    "    (\"  • Sees all ref_codes\", \"\", \"→ how to say it (voice)\"),\n",
    "    (\"  • Sees s_1...s_{t-1}\", \"\", \"→ continuity/prosody\"),\n",
    "    (\"  • CANNOT see s_{t+1}\", \"\", \"→ truly autoregressive\"),\n",
    "]\n",
    "\n",
    "y = 0.98\n",
    "for row in explanation:\n",
    "    text = f\"{row[0]:22s} {row[1]:32s} {row[2]}\"\n",
    "    ax2.text(0.02, y, text, transform=ax2.transAxes,\n",
    "             fontsize=8.5, fontfamily='monospace', verticalalignment='top')\n",
    "    y -= 0.07\n",
    "\n",
    "ax2.set_title(\"Attention Pattern Analysis\\n(VieNeu-TTS sequence regions)\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"causal_attention_mask.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe lower-triangular structure guarantees autoregressive generation.\")\n",
    "print(\"Generated tokens can attend to all reference codes → voice conditioning works.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch05-temp-header",
   "metadata": {},
   "source": [
    "## 4. Temperature Sampling — Effect on Vietnamese Tones\n",
    "\n",
    "Temperature $\\tau$ controls how \"sharp\" or \"flat\" the output distribution is:\n",
    "\n",
    "$$P'(s_t = v) \\propto P(s_t = v)^{1/\\tau}$$\n",
    "\n",
    "- $\\tau < 1$: more concentrated → less variation, monotone prosody\n",
    "- $\\tau = 1$: sample from model's learned distribution\n",
    "- $\\tau > 1$: more spread → more varied, higher risk of **tone errors** in Vietnamese\n",
    "\n",
    "For Vietnamese, tone errors are catastrophic: \"ma\" (ghost), \"má\" (mother), \"mà\" (but), \"mả\" (tomb), \"mã\" (code), \"mạ\" (rice seedling) all sound similar but differ in tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch05-temp-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def apply_temperature(logits, tau):\n",
    "    return softmax(logits / tau)\n",
    "\n",
    "# Simulate logit distribution with a realistic structure:\n",
    "# - A cluster of \"correct\" tokens for the intended tone\n",
    "# - A cluster of \"wrong tone\" tokens\n",
    "# - Background noise\n",
    "np.random.seed(42)\n",
    "vocab_size = 200\n",
    "logits = np.random.randn(vocab_size) - 3.0  # low baseline\n",
    "\n",
    "# Correct tone cluster (e.g., flat tone \"ma\"): tokens 20-30 are high probability\n",
    "logits[20:30] += np.array([4.5, 4.2, 3.8, 4.0, 3.5, 4.1, 3.7, 3.9, 3.6, 3.4])\n",
    "\n",
    "# Wrong tone cluster (e.g., rising tone \"má\"): tokens 80-90 are moderately probable\n",
    "logits[80:90] += np.array([2.0, 1.8, 1.5, 1.7, 1.6, 1.4, 1.9, 1.3, 1.5, 1.2])\n",
    "\n",
    "# Another wrong tone (falling \"mà\"): tokens 140-148\n",
    "logits[140:148] += np.array([1.2, 1.0, 0.8, 1.1, 0.9, 0.7, 1.0, 0.6])\n",
    "\n",
    "temperatures = [0.5, 0.8, 1.0, 1.5]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 9))\n",
    "\n",
    "for ax, tau in zip(axes.flatten(), temperatures):\n",
    "    probs = apply_temperature(logits, tau)\n",
    "    \n",
    "    # Compute probability in each tone cluster\n",
    "    p_correct = probs[20:30].sum()\n",
    "    p_wrong_1 = probs[80:90].sum()\n",
    "    p_wrong_2 = probs[140:148].sum()\n",
    "    p_other   = 1 - p_correct - p_wrong_1 - p_wrong_2\n",
    "    \n",
    "    # Plot probability distribution\n",
    "    colors = ['gray'] * vocab_size\n",
    "    for i in range(20, 30): colors[i] = '#2ecc71'   # correct = green\n",
    "    for i in range(80, 90): colors[i] = '#e74c3c'   # wrong tone 1 = red\n",
    "    for i in range(140, 148): colors[i] = '#e67e22' # wrong tone 2 = orange\n",
    "    \n",
    "    ax.bar(range(vocab_size), probs, color=colors, alpha=0.8, width=1.0)\n",
    "    ax.set_title(f\"Temperature τ = {tau}\", fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel(\"Token ID\", fontsize=9)\n",
    "    ax.set_ylabel(\"Probability\", fontsize=9)\n",
    "    \n",
    "    # Entropy\n",
    "    entropy = -np.sum(probs * np.log(probs + 1e-12))\n",
    "    \n",
    "    # Annotation box\n",
    "    info = (f\"P(correct tone) = {p_correct:.3f}\\n\"\n",
    "            f\"P(wrong tone 1) = {p_wrong_1:.3f}\\n\"\n",
    "            f\"P(wrong tone 2) = {p_wrong_2:.3f}\\n\"\n",
    "            f\"Entropy H = {entropy:.2f} nats\")\n",
    "    ax.text(0.62, 0.95, info, transform=ax.transAxes, fontsize=8,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    # Legend patches\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='#2ecc71', label='Correct tone (\"ma\")', alpha=0.8),\n",
    "        Patch(facecolor='#e74c3c', label='Wrong tone (\"má\")', alpha=0.8),\n",
    "        Patch(facecolor='#e67e22', label='Wrong tone (\"mà\")', alpha=0.8),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, fontsize=7, loc='upper left')\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Temperature Effect on Vietnamese Tone Token Distribution\\n\"\n",
    "    \"(VieNeu-TTS default: τ = 1.0 — balances prosody variation vs tone accuracy)\",\n",
    "    fontsize=12, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature_sampling.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nTemperature vs Tone Error Probability (simulated):\")\n",
    "print(f\"{'τ':>6} {'P(correct)':>12} {'P(error)':>12} {'Entropy':>10} {'Verdict':>20}\")\n",
    "print(\"-\" * 65)\n",
    "verdicts = {0.5: \"Too rigid / monotone\", 0.8: \"Good\", 1.0: \"VieNeu default\", 1.5: \"Risky for tones\"}\n",
    "for tau in temperatures:\n",
    "    probs = apply_temperature(logits, tau)\n",
    "    p_correct = probs[20:30].sum()\n",
    "    p_error = probs[80:90].sum() + probs[140:148].sum()\n",
    "    entropy = -np.sum(probs * np.log(probs + 1e-12))\n",
    "    print(f\"{tau:>6.1f} {p_correct:>12.4f} {p_error:>12.4f} {entropy:>10.3f} {verdicts[tau]:>20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch05-inference-header",
   "metadata": {},
   "source": [
    "## 5. Running Full Inference with VieNeu-TTS\n",
    "\n",
    "Now we put it all together: load the VieNeu-TTS model and synthesize Vietnamese text. We test:\n",
    "1. Short utterance: a simple greeting\n",
    "2. Medium utterance: a common sentence\n",
    "3. Long utterance: a technical sentence with loanwords\n",
    "\n",
    "For each, we measure the **Real-Time Factor (RTF)**:\n",
    "$$\\text{RTF} = \\frac{\\text{wall-clock inference time (s)}}{\\text{generated audio duration (s)}}$$\n",
    "\n",
    "RTF < 1.0 means faster than real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch05-inference-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "test_texts = [\n",
    "    (\"short\",   \"Xin chào.\"),\n",
    "    (\"medium\",  \"Hôm nay thời tiết đẹp lắm.\"),\n",
    "    (\"long\",    \"Trí tuệ nhân tạo đang thay đổi cách chúng ta làm việc mỗi ngày.\"),\n",
    "    (\"tech\",    \"Mô hình ngôn ngữ lớn được huấn luyện trên hàng tỷ token văn bản.\"),\n",
    "]\n",
    "\n",
    "if VIENEU_AVAILABLE:\n",
    "    from IPython.display import Audio, display\n",
    "    \n",
    "    tts = Vieneu()  # loads default GGUF model\n",
    "    \n",
    "    results = []\n",
    "    print(\"=\" * 65)\n",
    "    print(\"VieNeu-TTS Inference Results\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    for label, text in test_texts:\n",
    "        t0 = time.time()\n",
    "        audio = tts.infer(text)\n",
    "        elapsed = time.time() - t0\n",
    "        \n",
    "        audio_np = np.array(audio) if not isinstance(audio, np.ndarray) else audio\n",
    "        duration = len(audio_np) / 24000\n",
    "        rtf = elapsed / duration\n",
    "        tokens_generated = int(duration * 50)  # 50 tok/sec\n",
    "        tok_per_sec = tokens_generated / elapsed\n",
    "        \n",
    "        results.append((label, text, duration, elapsed, rtf, tok_per_sec))\n",
    "        \n",
    "        print(f\"\\n[{label.upper()}] {text}\")\n",
    "        print(f\"  Audio duration:     {duration:.2f}s\")\n",
    "        print(f\"  Inference time:     {elapsed:.2f}s\")\n",
    "        print(f\"  RTF:                {rtf:.3f}x {'✓ real-time' if rtf < 1.0 else '✗ slower than real-time'}\")\n",
    "        print(f\"  Tokens generated:   ~{tokens_generated}\")\n",
    "        print(f\"  Token throughput:   {tok_per_sec:.1f} tok/sec\")\n",
    "        display(Audio(audio_np, rate=24000))\n",
    "    \n",
    "    tts.close()\n",
    "    \n",
    "    # Summary plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    labels = [r[0] for r in results]\n",
    "    rtfs = [r[4] for r in results]\n",
    "    durations = [r[2] for r in results]\n",
    "    \n",
    "    axes[0].bar(labels, rtfs, color=['#2ecc71' if r < 1.0 else '#e74c3c' for r in rtfs])\n",
    "    axes[0].axhline(1.0, color='black', linestyle='--', linewidth=1.5, label='Real-time boundary')\n",
    "    axes[0].set_ylabel(\"Real-Time Factor (RTF)\")\n",
    "    axes[0].set_title(\"RTF by Utterance Type\\n(< 1.0 = faster than real-time)\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].scatter(durations, [r[5] for r in results], s=100, color='steelblue')\n",
    "    for r in results:\n",
    "        axes[1].annotate(r[0], (r[2], r[5]), textcoords='offset points', xytext=(5, 5))\n",
    "    axes[1].set_xlabel(\"Audio duration (s)\")\n",
    "    axes[1].set_ylabel(\"Token throughput (tok/sec)\")\n",
    "    axes[1].set_title(\"Throughput vs Audio Length\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"inference_rtf.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"[DEMO MODE] VieNeu-TTS not installed. Showing expected performance figures.\")\n",
    "    print()\n",
    "    print(\"Expected RTF values (Apple M2 Pro, GGUF Q4_K_M):\")\n",
    "    print(f\"{'Label':>8} {'Text':>55} {'Duration':>9} {'RTF':>6}\")\n",
    "    print(\"-\" * 85)\n",
    "    expected = [\n",
    "        (\"short\",  \"Xin chào.\",                                                   1.0, 0.65),\n",
    "        (\"medium\", \"Hôm nay thời tiết đẹp lắm.\",                                  2.5, 0.72),\n",
    "        (\"long\",   \"Trí tuệ nhân tạo đang thay đổi cách chúng ta làm việc...\",   4.8, 0.78),\n",
    "        (\"tech\",   \"Mô hình ngôn ngữ lớn được huấn luyện trên hàng tỷ token...\", 5.2, 0.80),\n",
    "    ]\n",
    "    for label, text, dur, rtf in expected:\n",
    "        print(f\"{label:>8} {text[:55]:>55} {dur:>7.1f}s {rtf:>6.2f}x\")\n",
    "    print()\n",
    "    print(\"Note: First call is slower (model loading). Subsequent calls achieve these RTFs.\")\n",
    "    print(\"CUDA GPU: RTF ≈ 0.05-0.15x (5-20x faster than real-time)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch05-kvcache-header",
   "metadata": {},
   "source": [
    "## 6. KV-Cache — Why It Speeds Up Generation\n",
    "\n",
    "Without KV-cache, generating token $t$ requires computing attention over all $t$ previous tokens from scratch. This is $O(t)$ per token, or $O(T^2)$ total.\n",
    "\n",
    "The KV-cache stores past keys and values. For token $t$, we only compute:\n",
    "- The new query $q_t$ (for position $t$)\n",
    "- Dot product $q_t \\cdot K_{1:t}$ using cached $K_{1:t}$\n",
    "\n",
    "This reduces per-token cost to $O(1)$ in compute (amortized), with $O(T)$ memory.\n",
    "\n",
    "**Memory formula for VieNeu-TTS-0.3B:**\n",
    "$$\\text{KV-cache size} = 2 \\times L \\times T \\times d_k \\times h \\times \\text{bytes/float}$$\n",
    "\n",
    "with $L=28$, $d_k=64$, $h=16$, float16 → $2 \\times 28 \\times T \\times 64 \\times 16 \\times 2 = 114{,}688 \\times T$ bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch05-kvcache-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# VieNeu-TTS-0.3B architecture params\n",
    "L = 28       # transformer layers\n",
    "h = 16       # attention heads\n",
    "d_k = 64     # key dimension per head\n",
    "bytes_per_float = 2  # float16\n",
    "\n",
    "# KV-cache size in MB as function of sequence length\n",
    "def kv_cache_mb(T):\n",
    "    # 2 (K and V) × L layers × T tokens × d_k × h heads × bytes\n",
    "    return 2 * L * T * d_k * h * bytes_per_float / (1024 ** 2)\n",
    "\n",
    "T_values = np.arange(1, 2049)\n",
    "cache_sizes = [kv_cache_mb(T) for T in T_values]\n",
    "\n",
    "# Compute FLOPs comparison: with vs without KV-cache\n",
    "# Without: generating T tokens needs T*(T+1)/2 attention computations\n",
    "# With: generating T tokens needs T attention computations (one per new token)\n",
    "def flops_without_cache(T):\n",
    "    return sum(t for t in range(1, T + 1))  # = T*(T+1)/2\n",
    "\n",
    "def flops_with_cache(T):\n",
    "    return T  # one attention computation per new token\n",
    "\n",
    "T_range = np.arange(1, 501)\n",
    "flops_no = [flops_without_cache(T) for T in T_range]\n",
    "flops_yes = [flops_with_cache(T) for T in T_range]\n",
    "speedup = [flops_without_cache(T) / flops_with_cache(T) for T in T_range]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: KV-cache memory\n",
    "axes[0].plot(T_values, cache_sizes, color='steelblue', linewidth=2)\n",
    "axes[0].axvline(250, color='orange', linestyle='--', label='250 tokens (5s speech)')\n",
    "axes[0].axvline(150, color='green', linestyle='--', label='150 tokens (3s ref)')\n",
    "axes[0].set_xlabel(\"Sequence length T (tokens)\", fontsize=10)\n",
    "axes[0].set_ylabel(\"KV-cache size (MB)\", fontsize=10)\n",
    "axes[0].set_title(f\"KV-Cache Memory\\n(VieNeu-TTS-0.3B, float16)\", fontsize=10)\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "# Annotate specific points\n",
    "for T in [150, 250, 500, 2048]:\n",
    "    axes[0].annotate(f\"{kv_cache_mb(T):.1f} MB\",\n",
    "                     (T, kv_cache_mb(T)), textcoords='offset points',\n",
    "                     xytext=(10, 5), fontsize=7, color='navy')\n",
    "\n",
    "# Plot 2: FLOPs comparison\n",
    "axes[1].plot(T_range, flops_no, label='Without KV-cache', color='crimson', linewidth=2)\n",
    "axes[1].plot(T_range, flops_yes, label='With KV-cache', color='#2ecc71', linewidth=2)\n",
    "axes[1].set_xlabel(\"Tokens generated T\", fontsize=10)\n",
    "axes[1].set_ylabel(\"Relative attention FLOPs\", fontsize=10)\n",
    "axes[1].set_title(\"Attention Compute: With vs Without KV-Cache\", fontsize=10)\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Speedup\n",
    "axes[2].plot(T_range, speedup, color='purple', linewidth=2)\n",
    "axes[2].fill_between(T_range, 1, speedup, alpha=0.2, color='purple')\n",
    "axes[2].axhline(1, color='black', linewidth=1, linestyle='--')\n",
    "axes[2].set_xlabel(\"Tokens generated T\", fontsize=10)\n",
    "axes[2].set_ylabel(\"Speedup factor (×)\", fontsize=10)\n",
    "axes[2].set_title(\"KV-Cache Speedup Factor = T/2 (approx)\", fontsize=10)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "# For T=200: speedup ≈ 100x\n",
    "T_ex = 200\n",
    "axes[2].annotate(f\"T={T_ex}: {speedup[T_ex-1]:.0f}× speedup\",\n",
    "                 (T_ex, speedup[T_ex-1]), textcoords='offset points',\n",
    "                 xytext=(20, -20), fontsize=9, color='purple',\n",
    "                 arrowprops=dict(arrowstyle='->', color='purple'))\n",
    "\n",
    "plt.suptitle(\"KV-Cache Analysis for VieNeu-TTS\", fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"kv_cache_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"KV-cache memory at key sequence lengths:\")\n",
    "for T in [100, 150, 250, 500, 1000, 2048]:\n",
    "    mb = kv_cache_mb(T)\n",
    "    description = \"\"\n",
    "    if T == 150: description = \" ← 3s reference audio\"\n",
    "    if T == 250: description = \" ← 5s generated speech\"\n",
    "    if T == 2048: description = \" ← full context\"\n",
    "    print(f\"  T={T:5d}: {mb:6.2f} MB{description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch05-summary",
   "metadata": {},
   "source": [
    "## Summary — VieNeu-TTS Prompt Format Cheat Sheet\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│  VieNeu-TTS INFERENCE PROMPT STRUCTURE                                      │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│  user: Convert the text to speech:                                          │\n",
    "│  <|TEXT_PROMPT_START|>                                                      │\n",
    "│      {ref_text_phonemes}           ← phonemized reference transcript        │\n",
    "│      {SPACE}                                                                │\n",
    "│      {input_text_phonemes}         ← phonemized synthesis target            │\n",
    "│  <|TEXT_PROMPT_END|>                                                        │\n",
    "│  assistant:<|SPEECH_GENERATION_START|>                                      │\n",
    "│      {ref_codes}                   ← DistillNeuCodec tokens from ref audio  │\n",
    "│  ──── model generates below this line ────────────────────────────────────  │\n",
    "│      {generated_codes}             ← autoregressive speech tokens           │\n",
    "│  <|SPEECH_GENERATION_END|>         ← stop condition                         │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "| Parameter | Value | Effect |\n",
    "|---|---|---|\n",
    "| Codec | DistillNeuCodec | 50 tokens/sec |\n",
    "| Temperature | τ = 1.0 | Natural prosody, safe for tones |\n",
    "| Top-k | k = 50 | Prevents acoustic glitches |\n",
    "| Context | 2048 tokens | ~36s max generation (3s ref) |\n",
    "| KV-cache | Enabled | O(1) per-token compute |\n",
    "| Quantization | GGUF Q4_K_M | ~1.4x RTF on M2 CPU |\n",
    "\n",
    "**Key equation:** $\\mathcal{L} = -\\sum_{t=1}^{T} \\log P_\\theta(s_t \\mid s_{<t}, \\mathbf{x})$\n",
    "\n",
    "**Next chapter:** Zero-shot voice cloning — how the model learns to clone voices it has never trained on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "# Chapter 3 — TTS Architecture Evolution\n",
    "\n",
    "This notebook accompanies **Chapter 03** of the VieNeu-TTS deep learning guide.  \n",
    "We trace the evolution of TTS systems from concatenative (1990s) to LLM-based (2023+), with hands-on demonstrations at each step.\n",
    "\n",
    "**What we cover**:\n",
    "1. The fundamental problem: text → waveform\n",
    "2. Why concatenative TTS struggles with Vietnamese tones\n",
    "3. HMM parametric TTS — visualizing over-smoothing\n",
    "4. Tacotron 2 — attention alignment\n",
    "5. FastSpeech 2 — duration-based phoneme alignment\n",
    "6. LLM-TTS — speech as token sequences\n",
    "7. Architecture comparison table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport librosa\nimport librosa.display\nimport soundfile as sf\nfrom IPython.display import Audio, display\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['font.size'] = 11\n\nprint(f\"librosa version: {librosa.__version__}\")\nprint(f\"numpy version:   {np.__version__}\")\nprint(\"All imports successful.\")\n\n# ── Path resolver (works regardless of Jupyter CWD) ──────────────\nfrom pathlib import Path\nimport os as _os\n\ndef _find_examples_dir():\n    # Walk up from CWD (works locally when kernel starts in chapters/)\n    for _p in [Path(_os.getcwd())] + list(Path(_os.getcwd()).parents):\n        _d = _p / \"examples\" / \"audio_ref\"\n        if _d.is_dir():\n            return _d\n    # Colab fallback paths\n    for _candidate in [\n        Path(\"/content/vietnamese-tts-course/examples/audio_ref\"),\n        Path(\"/content/VieNeu-TTS/examples/audio_ref\"),\n    ]:\n        if _candidate.is_dir():\n            return _candidate\n    raise FileNotFoundError(\n        \"examples/audio_ref/ not found. \"\n        \"Clone the repo: git clone https://github.com/thinhdanggroup/vietnamese-tts-course.git\"\n    )\n\nEXAMPLES_DIR = _find_examples_dir()\nprint(f\"Examples dir: {EXAMPLES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3e4f5",
   "metadata": {},
   "source": [
    "## 1. The Problem: Text to Waveform\n",
    "\n",
    "TTS is fundamentally a **sequence-to-sequence** mapping problem:\n",
    "\n",
    "$$\\text{Text} \\rightarrow \\text{Phonemes} \\rightarrow \\underbrace{\\text{Mel Spectrogram}_{80 \\times T}}_\\text{intermediate} \\rightarrow \\text{Waveform}_{T'}$$\n",
    "\n",
    "The difficulty: text is **abstract** (discrete symbols, no acoustic information) while speech is **rich and continuous** (thousands of samples per second with complex harmonic structure, tone, prosody, speaker identity).\n",
    "\n",
    "We visualize what the TTS model must predict from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Vietnamese audio and show what TTS must produce\naudio_path = str(EXAMPLES_DIR / \"example.wav\")\ny, sr = librosa.load(audio_path, sr=None)\n\n# Compute mel spectrogram\nn_fft = 1024\nhop_length = 256\nn_mels = 80\n\nmel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft,\n                                           hop_length=hop_length, n_mels=n_mels)\nmel_db = librosa.power_to_db(mel_spec, ref=np.max)\n\n# Create the visual explanation\nfig = plt.figure(figsize=(16, 9))\ngs = fig.add_gridspec(3, 3, hspace=0.5, wspace=0.4)\n\n# --- Text input (conceptual) ---\nax_text = fig.add_subplot(gs[0, 0])\nax_text.axis('off')\ntext_example = \"Xin chào\\nViệt Nam.\"\nax_text.text(0.5, 0.5, text_example, ha='center', va='center', fontsize=16,\n             bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\nax_text.set_title(\"1. Input: Text\", fontweight='bold', color='navy')\n\n# --- Phonemes ---\nax_ph = fig.add_subplot(gs[0, 1])\nax_ph.axis('off')\nphonemes = \"s i n\\ntɕ aː w\\nv iɛ t\\nn a m\"\nax_ph.text(0.5, 0.5, phonemes, ha='center', va='center', fontsize=13, family='monospace',\n           bbox=dict(boxstyle='round,pad=0.5', facecolor='lightyellow', alpha=0.8))\nax_ph.set_title(\"2. Phonemes (G2P)\", fontweight='bold', color='darkgreen')\n\n# --- Token IDs ---\nax_tok = fig.add_subplot(gs[0, 2])\nax_tok.axis('off')\ntokens = \"[847, 312, 1205,\\n 891, 45, 567,\\n 2341, 78, 1024,\\n 412, 890, 2341]\"\nax_tok.text(0.5, 0.5, tokens, ha='center', va='center', fontsize=11, family='monospace',\n            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.8))\nax_tok.set_title(\"3. Token IDs (Tokenizer)\", fontweight='bold', color='darkgreen')\n\n# --- Waveform ---\nax_wave = fig.add_subplot(gs[1, :])\nlibrosa.display.waveshow(y, sr=sr, ax=ax_wave, color='steelblue', alpha=0.8)\nax_wave.set_title(\"4. Waveform — what TTS must ultimately produce\", fontweight='bold')\nax_wave.set_xlabel(\"Time (s)\")\n\n# --- Mel Spectrogram ---\nax_mel = fig.add_subplot(gs[2, :])\nimg = librosa.display.specshow(mel_db, sr=sr, hop_length=hop_length,\n                                x_axis='time', y_axis='mel',\n                                ax=ax_mel, cmap='magma')\nax_mel.set_title(\"5. Mel Spectrogram — the intermediate target for most neural TTS systems\",\n                 fontweight='bold')\nfig.colorbar(img, ax=ax_mel, format='%+2.0f dB')\n\nplt.suptitle(\"The TTS Problem: Map discrete text symbols → continuous speech waveform\",\n             fontsize=13, fontweight='bold', y=1.01)\nplt.savefig('ch03_problem.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nAudio properties:\")\nprint(f\"  Sample rate:         {sr} Hz\")\nprint(f\"  Duration:            {len(y)/sr:.2f} s\")\nprint(f\"  Waveform samples:    {len(y):,}\")\nprint(f\"  Mel spec shape:      {mel_spec.shape}\")\nprint(f\"\")\nprint(f\"The TTS model must predict {mel_spec.shape[0]}×{mel_spec.shape[1]} = {mel_spec.shape[0]*mel_spec.shape[1]:,} values\")\nprint(f\"from a ~15 character input — a massive expansion ratio!\")\n\ndisplay(Audio(y, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5a6b7",
   "metadata": {},
   "source": [
    "## 2. Why Concatenative TTS Struggles with Vietnamese Tones\n",
    "\n",
    "Concatenative TTS stores recordings of real speech units (phonemes, diphones, syllables) and concatenates them. For Vietnamese, the 6-tone system **multiplies the required inventory size** dramatically compared to non-tonal languages like English.\n",
    "\n",
    "The **target cost** + **join cost** dynamic programming formulation:\n",
    "$$C_{\\text{total}} = \\sum_i C_T(t_i, u_i) + \\sum_i C_J(u_i, u_{i+1})$$\n",
    "\n",
    "must search over a huge unit inventory, and rare tone combinations may not exist in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the tonal inventory size problem\n",
    "\n",
    "# Vietnamese phonological inventory\n",
    "tones             = 6    # The 6 Vietnamese tones\n",
    "vowel_nuclei      = 12   # Main vowels: a, ă, â, e, ê, i, o, ô, ơ, u, ư + schwa\n",
    "onset_consonants  = 23   # Initial consonants (including zero onset)\n",
    "final_consonants  = 8    # Final consonants (p, t, k, m, n, ng, j, w)\n",
    "no_final          = 1    # Open syllables (no final consonant)\n",
    "\n",
    "# Syllable inventory size (tone-aware)\n",
    "syllable_types_notonal = onset_consonants * vowel_nuclei * (final_consonants + no_final)\n",
    "syllable_types_tonal   = syllable_types_notonal * tones\n",
    "\n",
    "# English comparison (no tones)\n",
    "eng_onsets  = 25   # Approximate English onset consonant clusters\n",
    "eng_nuclei  = 15   # English vowels/diphthongs\n",
    "eng_codas   = 30   # English coda clusters (much more complex than Vietnamese)\n",
    "eng_syllables = eng_onsets * eng_nuclei * eng_codas\n",
    "\n",
    "# Diphone inventory (for tone-aware diphone synthesis)\n",
    "# Each diphone = transition from center of syllable_i to center of syllable_{i+1}\n",
    "# For tone-aware Vietnamese: need diphones for all 6×6 = 36 tone pair combinations\n",
    "tone_pair_combos    = tones * tones  # 36 possible tone transitions\n",
    "base_diphones       = onset_consonants * vowel_nuclei  # Simplified: onset-nucleus transitions\n",
    "viet_diphone_est    = base_diphones * tone_pair_combos\n",
    "eng_diphone_est     = eng_onsets * eng_nuclei  # No tone pairs for English\n",
    "\n",
    "# Recording requirements (0.5s per unit, 50% coverage of inventory)\n",
    "units_needed_viet   = syllable_types_tonal // 10  # Only 10% of units actually recorded in practice\n",
    "recording_time_viet = units_needed_viet * 0.5  # seconds\n",
    "recording_time_eng  = eng_syllables // 5 * 0.5\n",
    "\n",
    "print(\"Concatenative TTS Inventory Analysis\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Vietnamese phonological inventory:\")\n",
    "print(f\"  Tones:                   {tones}\")\n",
    "print(f\"  Onset consonants:        {onset_consonants}\")\n",
    "print(f\"  Vowel nuclei:            {vowel_nuclei}\")\n",
    "print(f\"  Final consonants:        {final_consonants} + {no_final} (open)\")\n",
    "print()\n",
    "print(f\"Syllable types (no tones): {syllable_types_notonal:,}\")\n",
    "print(f\"Syllable types (tonal):    {syllable_types_tonal:,}  ← {tones}× more than toneless\")\n",
    "print()\n",
    "print(f\"Tone pair combinations:    {tones} × {tones} = {tone_pair_combos}  (e.g., flat→falling, rising→dipping)\")\n",
    "print(f\"Vietnamese diphone est.:   {viet_diphone_est:,}\")\n",
    "print(f\"English diphone est.:      {eng_diphone_est:,}\")\n",
    "print(f\"Ratio:                     {viet_diphone_est/eng_diphone_est:.1f}× more units for Vietnamese\")\n",
    "print()\n",
    "print(f\"Recording time (Vietnamese, 10% coverage): {recording_time_viet/3600:.1f} hours\")\n",
    "print(f\"Recording time (English,    20% coverage): {recording_time_eng/3600:.1f} hours\")\n",
    "print()\n",
    "\n",
    "# Visualize the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar chart: inventory sizes\n",
    "ax = axes[0]\n",
    "categories = ['English\\n(no tones)', 'Vietnamese\\n(no tones)', 'Vietnamese\\n(6 tones)']\n",
    "values = [eng_syllables, syllable_types_notonal, syllable_types_tonal]\n",
    "colors_bar = ['#2196F3', '#FF9800', '#F44336']\n",
    "bars = ax.bar(categories, values, color=colors_bar, alpha=0.8, edgecolor='black')\n",
    "for bar, val in zip(bars, values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100,\n",
    "            f'{val:,}', ha='center', fontweight='bold')\n",
    "ax.set_title(\"Syllable Inventory Size\\n(Concatenative TTS requires covering this space)\",\n",
    "             fontweight='bold')\n",
    "ax.set_ylabel(\"Number of distinct syllable types\")\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Tone pair matrix: show which transitions exist\n",
    "ax2 = axes[1]\n",
    "tone_names_short = ['Ngang', 'Huyền', 'Sắc', 'Hỏi', 'Ngã', 'Nặng']\n",
    "\n",
    "# Simulate tone pair frequency in natural Vietnamese speech\n",
    "np.random.seed(42)\n",
    "# Not all 36 pairs are equally common\n",
    "tone_pair_freq = np.random.dirichlet(np.ones(36)).reshape(6, 6)\n",
    "tone_pair_freq /= tone_pair_freq.max()\n",
    "\n",
    "im = ax2.imshow(tone_pair_freq, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_xticks(range(6))\n",
    "ax2.set_yticks(range(6))\n",
    "ax2.set_xticklabels(tone_names_short, rotation=45, ha='right', fontsize=9)\n",
    "ax2.set_yticklabels(tone_names_short, fontsize=9)\n",
    "ax2.set_xlabel(\"Next syllable tone\")\n",
    "ax2.set_ylabel(\"Current syllable tone\")\n",
    "ax2.set_title(\"Tone Pair Transition Matrix\\n(Concatenative TTS needs units for ALL 36 pairs)\",\n",
    "              fontweight='bold')\n",
    "fig.colorbar(im, ax=ax2, label='Relative frequency')\n",
    "\n",
    "# Annotate each cell\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        ax2.text(j, i, f'{i+1}→{j+1}', ha='center', va='center', fontsize=7, color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch03_concatenative.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Conclusion: Vietnamese concatenative TTS needs ~10× more recorded units than English.\")\n",
    "print(\"And the join cost computation becomes very expensive with tone context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "## 3. HMM Parametric TTS — Visualizing Over-Smoothing\n",
    "\n",
    "HMM-TTS generates acoustic features by taking the **Maximum Likelihood** estimate from the emission distribution. This MLPG solution is the mean, which over-smooths the spectrum:\n",
    "\n",
    "$$\\hat{\\mathbf{c}} = \\arg\\max_\\mathbf{c} \\sum_t \\log \\mathcal{N}(\\mathbf{o}_t; W_t \\mathbf{c}, \\Sigma) = (W^T \\Sigma^{-1} W)^{-1} W^T \\Sigma^{-1} \\boldsymbol{\\mu}$$\n",
    "\n",
    "The result: formant transitions are **blurred**, tone F0 contours are **rounded off** → the characteristic \"buzzy\" quality of HMM-TTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate HMM over-smoothing with Gaussian blur on mel spectrogram\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Load reference mel spectrogram\n",
    "mel_real = librosa.power_to_db(\n",
    "    librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels),\n",
    "    ref=np.max\n",
    ")\n",
    "\n",
    "# Simulate HMM over-smoothing: apply Gaussian blur\n",
    "# sigma_time: smoothing in time direction (duration modeling)\n",
    "# sigma_freq: smoothing in frequency direction (spectral over-smoothing)\n",
    "sigma_time = 2.0\n",
    "sigma_freq = 2.5\n",
    "mel_hmm = gaussian_filter(mel_real, sigma=[sigma_freq, sigma_time])\n",
    "\n",
    "# Also show an intermediate amount of smoothing\n",
    "mel_mild = gaussian_filter(mel_real, sigma=[0.8, 0.5])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, mel_data, title, cmap_use in [\n",
    "    (axes[0], mel_real, \"Real Vietnamese Speech\\n(Neural TTS target)\", 'magma'),\n",
    "    (axes[1], mel_mild, \"Mild Over-Smoothing\\n(FastSpeech 2 / Tacotron 2 approximation)\", 'magma'),\n",
    "    (axes[2], mel_hmm,  \"Heavy Over-Smoothing\\n(HMM-TTS: MLPG over-smoothing)\", 'magma'),\n",
    "]:\n",
    "    img = librosa.display.specshow(\n",
    "        mel_data, sr=sr, hop_length=hop_length,\n",
    "        x_axis='time', y_axis='mel',\n",
    "        ax=ax, cmap=cmap_use, vmin=-80, vmax=0\n",
    "    )\n",
    "    ax.set_title(title, fontweight='bold', fontsize=10)\n",
    "    ax.set_ylim(0, 8000)\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "\n",
    "# Annotate key differences\n",
    "for ax_idx, ax in enumerate(axes):\n",
    "    # Mark formant region\n",
    "    ax.axhline(500, color='cyan', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    ax.axhline(1500, color='cyan', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    if ax_idx == 0:\n",
    "        ax.text(0.02, 0.3, 'F1\\nF2', transform=ax.transAxes, color='cyan', fontsize=8)\n",
    "\n",
    "plt.suptitle(\"HMM Over-Smoothing: MLPG produces the statistical mean → blurs formants and tone contour\",\n",
    "             fontsize=11, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch03_hmm_smoothing.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Quantify the smoothing\n",
    "variance_real = np.var(mel_real)\n",
    "variance_hmm  = np.var(mel_hmm)\n",
    "print(f\"Spectral variance — Real:         {variance_real:.2f} dB²\")\n",
    "print(f\"Spectral variance — HMM simulated: {variance_hmm:.2f} dB²\")\n",
    "print(f\"Variance reduction:               {100*(1-variance_hmm/variance_real):.1f}%\")\n",
    "print()\n",
    "print(\"Effect on Vietnamese tones:\")\n",
    "print(\"  F0 contours are smoothed → tones sound less distinct\")\n",
    "print(\"  Tone hỏi (dipping) loses its inflection point\")\n",
    "print(\"  Tone ngã loses its creaky break\")\n",
    "print(\"  Result: 'buzzy', robotic sound characteristic of HMM-TTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "## 4. Tacotron 2 — Attention Alignment\n",
    "\n",
    "Tacotron 2 uses **location-sensitive attention** to learn the alignment between phoneme positions and mel spectrogram frames:\n",
    "\n",
    "$$e_{i,j} = v^T \\tanh\\!\\left(W_q \\mathbf{s}_i + V_k \\mathbf{h}_j + U \\mathbf{f}_{i,j} + \\mathbf{b}\\right)$$\n",
    "$$\\alpha_{i,j} = \\text{softmax}_j(e_{i,j})$$\n",
    "\n",
    "A **good alignment** is a near-diagonal matrix: decoder frame $i$ should attend mostly to encoder position $j \\approx i \\cdot n/T$ (moving left-to-right through the input at a steady pace).\n",
    "\n",
    "**Attention failure modes** are common for Vietnamese tones — especially at syllable boundaries where tone changes create prosodic discontinuities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate attention alignment matrices — good vs. failure modes\n",
    "\n",
    "def make_good_alignment(n_phonemes, n_mel_frames, speed_variation=0.1):\n",
    "    \"\"\"Create a synthetic 'good' attention alignment: near-diagonal with slight noise.\"\"\"\n",
    "    alignment = np.zeros((n_mel_frames, n_phonemes))\n",
    "    # Expected alignment: frame i should attend to phoneme j = i * n_phonemes/n_mel_frames\n",
    "    for i in range(n_mel_frames):\n",
    "        j_center = i * n_phonemes / n_mel_frames\n",
    "        # Add some sharpness to attention peaks\n",
    "        for j in range(n_phonemes):\n",
    "            dist = abs(j - j_center)\n",
    "            alignment[i, j] = np.exp(-dist**2 / (2 * 0.8**2))\n",
    "        alignment[i] /= alignment[i].sum()\n",
    "    return alignment\n",
    "\n",
    "def make_stuck_alignment(n_phonemes, n_mel_frames, stuck_at=5, stuck_frames=20):\n",
    "    \"\"\"Simulate attention getting 'stuck' on one phoneme (repetition failure).\"\"\"\n",
    "    alignment = make_good_alignment(n_phonemes, n_mel_frames)\n",
    "    # Artificially keep attention on phoneme 'stuck_at' for 'stuck_frames' frames\n",
    "    for i in range(stuck_at * n_mel_frames // n_phonemes,\n",
    "                   stuck_at * n_mel_frames // n_phonemes + stuck_frames):\n",
    "        if i < n_mel_frames:\n",
    "            alignment[i] = 0\n",
    "            alignment[i, stuck_at] = 1.0\n",
    "    return alignment\n",
    "\n",
    "def make_skipping_alignment(n_phonemes, n_mel_frames, skip_phoneme=7):\n",
    "    \"\"\"Simulate attention skipping a phoneme (word dropped).\"\"\"\n",
    "    alignment = make_good_alignment(n_phonemes, n_mel_frames)\n",
    "    # Skip phoneme 'skip_phoneme' — attention jumps over it\n",
    "    skip_start_frame = skip_phoneme * n_mel_frames // n_phonemes\n",
    "    skip_end_frame   = (skip_phoneme + 1) * n_mel_frames // n_phonemes\n",
    "    for i in range(skip_start_frame, skip_end_frame):\n",
    "        if i < n_mel_frames:\n",
    "            alignment[i] = 0\n",
    "            alignment[i, skip_phoneme + 1] = 1.0\n",
    "    return alignment\n",
    "\n",
    "# Vietnamese phoneme sequence: \"xin chào Việt Nam\" (simplified)\n",
    "phoneme_seq = ['s', 'i', 'n', 'tɕ', 'aː', 'w', 'v', 'iɛ', 't', 'n', 'a', 'm']\n",
    "n_phonemes  = len(phoneme_seq)\n",
    "n_frames    = 80  # ~80 mel frames for this utterance\n",
    "\n",
    "alignments = {\n",
    "    \"Good Alignment\\n(diagonal — correct left-to-right progress)\": make_good_alignment(n_phonemes, n_frames),\n",
    "    \"Stuck Alignment\\n(repetition: stuck on 'n' → repeated syllable)\": make_stuck_alignment(n_phonemes, n_frames, stuck_at=2, stuck_frames=15),\n",
    "    \"Skipping Alignment\\n(word dropped: jumps over 'w'→'v' boundary)\": make_skipping_alignment(n_phonemes, n_frames, skip_phoneme=5),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "title_colors = ['green', 'red', 'orange']\n",
    "\n",
    "for ax, (title, alignment), tcolor in zip(axes, alignments.items(), title_colors):\n",
    "    img = ax.imshow(alignment, aspect='auto', origin='lower', cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(title, fontweight='bold', color=tcolor, fontsize=10)\n",
    "    ax.set_xlabel(\"Phoneme position (input)\")\n",
    "    ax.set_ylabel(\"Mel frame (output, time)\")\n",
    "    ax.set_xticks(range(n_phonemes))\n",
    "    ax.set_xticklabels(phoneme_seq, fontsize=9, rotation=45)\n",
    "    fig.colorbar(img, ax=ax, fraction=0.046)\n",
    "\n",
    "# Highlight diagonal on first (good) plot\n",
    "diag_x = np.linspace(0, n_phonemes-1, n_frames)\n",
    "diag_y = np.arange(n_frames)\n",
    "axes[0].plot(diag_x, diag_y, 'r--', linewidth=1.5, alpha=0.5, label='Ideal diagonal')\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Tacotron 2 Attention Alignment Patterns for Vietnamese 'xin chào Việt Nam'\",\n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch03_attention.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Vietnamese-specific attention challenges:\")\n",
    "print(\"  - Tone boundaries can confuse attention (prosodic discontinuity)\")\n",
    "print(\"  - Long vowels (aː, iː) cause attention to 'pause' longer on one position\")\n",
    "print(\"  - Glottal stops in tone ngã can cause premature attention shift\")\n",
    "print(\"  - Location-sensitive attention HELPS by adding 'where was I' information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## 5. FastSpeech 2 — Duration Predictor\n",
    "\n",
    "FastSpeech 2 eliminates the attention mechanism entirely by using a **duration predictor** to determine how many mel frames each phoneme produces. The **length regulator** then copies each phoneme hidden state `d_i` times:\n",
    "\n",
    "$$\\text{LR}(\\mathbf{h}, \\mathbf{d}) = [\\underbrace{\\mathbf{h}_1, \\ldots, \\mathbf{h}_1}_{d_1}, \\underbrace{\\mathbf{h}_2, \\ldots, \\mathbf{h}_2}_{d_2}, \\ldots]$$\n",
    "\n",
    "For Vietnamese, the duration predictor must learn that:\n",
    "- Tone nặng → shorter duration (abruptly terminated)\n",
    "- Tone huyền → longer duration (sustained falling)\n",
    "- Aspirated consonants (th, kh, ph) → longer onset duration\n",
    "- Final stops (t, k, p) → very short closure duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate duration prediction concept for Vietnamese\n",
    "\n",
    "# Phoneme sequences and their predicted durations\n",
    "# \"Xin chào\" — two syllables, each with distinct phonemes and durations\n",
    "examples = [\n",
    "    {\n",
    "        \"utterance\": \"xin chào\",\n",
    "        \"phonemes\": [\"s\",  \"i\",  \"n\",  \"tɕ\", \"aː\", \"w\"],\n",
    "        \"durations\": [60,   80,   70,   90,   150,  120],  # ms\n",
    "        \"notes\":    [\"onset\",\"nucleus\",\"coda\",\"onset\",\"long vowel (huyền)\",\"glide\"]\n",
    "    },\n",
    "    {\n",
    "        \"utterance\": \"Việt Nam\",\n",
    "        \"phonemes\": [\"v\",  \"iɛ\", \"t\",  \"n\",  \"a\",  \"m\"],\n",
    "        \"durations\": [55,   100,  40,   60,   90,   110],\n",
    "        \"notes\":    [\"onset\",\"diphthong\",\"final stop (short)\",\"onset\",\"open vowel\",\"nasal coda\"]\n",
    "    },\n",
    "    {\n",
    "        \"utterance\": \"hệ thống\",\n",
    "        \"phonemes\": [\"h\",  \"e\",  \"tʰ\", \"oŋ\"],\n",
    "        \"durations\": [40,   80,   95,   140],\n",
    "        \"notes\":    [\"glottal\",\"short (hệ=nặng)\",\"aspirated onset\",\"nasal rhyme (hỏi)\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(examples), 1, figsize=(14, 3.5 * len(examples)))\n",
    "if len(examples) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "colors_ph = plt.cm.Set3(np.linspace(0, 1, 10))\n",
    "\n",
    "for ax, ex in zip(axes, examples):\n",
    "    phonemes  = ex[\"phonemes\"]\n",
    "    durations = ex[\"durations\"]\n",
    "    notes     = ex[\"notes\"]\n",
    "\n",
    "    x_pos = 0\n",
    "    for i, (ph, dur, note) in enumerate(zip(phonemes, durations, notes)):\n",
    "        color = colors_ph[i % len(colors_ph)]\n",
    "        rect = patches.Rectangle((x_pos, 0.15), dur, 0.7,\n",
    "                                   facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Phoneme symbol\n",
    "        ax.text(x_pos + dur/2, 0.5, ph,\n",
    "                ha='center', va='center', fontsize=13, fontweight='bold')\n",
    "\n",
    "        # Duration label\n",
    "        ax.text(x_pos + dur/2, 0.05, f'{dur}ms',\n",
    "                ha='center', va='center', fontsize=8, color='navy')\n",
    "\n",
    "        # Note below\n",
    "        ax.text(x_pos + dur/2, -0.15, note,\n",
    "                ha='center', va='top', fontsize=7, color='gray', rotation=30)\n",
    "\n",
    "        x_pos += dur\n",
    "\n",
    "    total_dur = sum(durations)\n",
    "    ax.set_xlim(-10, x_pos + 10)\n",
    "    ax.set_ylim(-0.5, 1.2)\n",
    "    ax.set_xlabel(\"Time (ms)\", fontsize=10)\n",
    "    ax.set_title(f\"Duration-based phoneme alignment: '{ex['utterance']}'  \"\n",
    "                 f\"[total: {total_dur}ms = {total_dur/1000:.2f}s]\",\n",
    "                 fontweight='bold', fontsize=11)\n",
    "    ax.set_yticks([])\n",
    "    ax.set_ylim(-0.6, 1.1)\n",
    "\n",
    "    # Add frame grid (mel frames at hop_length=256 @ 24kHz = 10.67ms)\n",
    "    frame_ms = 256 / 24000 * 1000  # ≈ 10.67ms\n",
    "    for frame_start in np.arange(0, x_pos, frame_ms):\n",
    "        ax.axvline(frame_start, color='gray', alpha=0.15, linewidth=0.5)\n",
    "\n",
    "    ax.text(x_pos - 50, 1.05, f'Frame grid: {frame_ms:.1f}ms (hop=256)',\n",
    "            fontsize=7, color='gray', ha='right')\n",
    "\n",
    "plt.suptitle(\"FastSpeech 2: Duration Predictor maps each phoneme to N mel frames\\n\"\n",
    "             \"Length Regulator copies the phoneme hidden state N times\",\n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch03_duration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"FastSpeech 2 advantages over Tacotron 2:\")\n",
    "print(\"  1. No attention → no attention failures (repetition, skipping)\")\n",
    "print(\"  2. All mel frames generated in parallel → 5-10× faster inference\")\n",
    "print(\"  3. Duration can be explicitly controlled → speech rate adjustment\")\n",
    "print(\"  4. Pitch predictor separately learns F0 contours → better tone control\")\n",
    "print()\n",
    "print(\"Vietnamese-specific benefit of explicit duration modeling:\")\n",
    "print(\"  Tone nặng → shorter duration (model learns this)\")\n",
    "print(\"  Tone huyền → longer, sustained duration\")\n",
    "print(\"  Duration predictor trained with MFA forced alignment on Vietnamese corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6",
   "metadata": {},
   "source": [
    "## 6. LLM-TTS — Speech as Token Sequence\n",
    "\n",
    "VieNeu-TTS (and the VALL-E / NeuTTS lineage) treat TTS as **next-token prediction**:\n",
    "\n",
    "$$P(\\text{speech}) = \\prod_t P(\\text{speech\\_token}_t | \\text{speech\\_token}_{<t}, \\text{text\\_tokens}, \\text{ref\\_speech\\_tokens})$$\n",
    "\n",
    "This is **identical** to the language modeling objective used to train LLMs — the only difference is that the vocabulary includes speech tokens (from NeuCodec) in addition to text tokens.\n",
    "\n",
    "Key insight: **zero-shot cloning** happens through in-context learning — the model has seen many (text, reference speech, target speech) triples during training, so at inference it can generalize to new speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c5d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "# Visualize how VieNeu-TTS represents speech as token IDs\n",
    "\n",
    "# Attempt to use the actual phonemizer\n",
    "try:\n",
    "    from vieneu_utils.phonemize_text import phonemize_with_dict\n",
    "    PHONEMIZER_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PHONEMIZER_AVAILABLE = False\n",
    "\n",
    "text = \"Xin chào Việt Nam.\"\n",
    "\n",
    "if PHONEMIZER_AVAILABLE:\n",
    "    phonemes = phonemize_with_dict(text)\n",
    "else:\n",
    "    phonemes = \"s i n tɕ aː w˨˩˦ v iɛ t˨˩ n a m˧\"  # expected output\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VieNeu-TTS: Unified Text + Speech Token Representation\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"Input text:  {text}\")\n",
    "print(f\"Phonemes:    {phonemes}\")\n",
    "print()\n",
    "print(\"Text tokens (from BPE tokenizer, approximate):\")\n",
    "text_words = text.split()\n",
    "# Simulated token IDs for Vietnamese words\n",
    "text_token_ids = [847, 2341, 1205, 3456, 12]  # fictional IDs\n",
    "for word, tid in zip(text_words, text_token_ids):\n",
    "    print(f\"  '{word}' → token_id={tid}\")\n",
    "\n",
    "print()\n",
    "print(\"After NeuCodec encoding, reference audio becomes:\")\n",
    "# Simulated NeuCodec tokens (first codebook)\n",
    "codec_tokens_ref = [234, 891, 45, 567, 123, 789, 456, 901, 34, 678, 235, 892]\n",
    "print(f\"  Reference speech tokens: {codec_tokens_ref}\")\n",
    "print(f\"  (NeuCodec: {75} tokens/second at 24 kHz)\")\n",
    "\n",
    "print()\n",
    "print(\"Generated speech tokens (model prediction):\")\n",
    "codec_tokens_gen = [412, 789, 56, 890, 234, 567, 891, 45, 678, 123, 456, 789]\n",
    "print(f\"  Generated speech tokens: {codec_tokens_gen}\")\n",
    "\n",
    "print()\n",
    "print(\"Full token sequence fed to LLM (concept):\")\n",
    "print(\"-\" * 60)\n",
    "full_sequence = (\n",
    "    \"[TEXT_START] 847 2341 1205 3456 12 [TEXT_END] \"\n",
    "    \"[SPEECH_START] 234 891 45 567 ... [SPEECH_END] \"\n",
    "    \"[SPEECH_GEN_START] → model predicts: 412 789 56 ... [SPEECH_GEN_END]\"\n",
    ")\n",
    "print(full_sequence)\n",
    "print()\n",
    "print(\"KEY INSIGHT:\")\n",
    "print(\"  Text tokens and speech tokens are in the SAME vocabulary.\")\n",
    "print(\"  The LLM learns to 'translate' text tokens → speech tokens.\")\n",
    "print(\"  This is standard language modeling: predict next token given context.\")\n",
    "print(\"  Zero-shot cloning = new speaker's tokens as context → model copies style.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the LLM-TTS architecture conceptually\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "ax.set_title(\"VieNeu-TTS Architecture: LLM + NeuCodec\", fontsize=14, fontweight='bold')\n",
    "\n",
    "def draw_box(ax, x, y, w, h, text, color, fontsize=9, text_color='white'):\n",
    "    rect = patches.FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.1\",\n",
    "                                   facecolor=color, edgecolor='black', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x+w/2, y+h/2, text, ha='center', va='center',\n",
    "            fontsize=fontsize, fontweight='bold', color=text_color,\n",
    "            multialignment='center')\n",
    "\n",
    "def draw_arrow(ax, x1, y1, x2, y2, label='', color='black'):\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=2))\n",
    "    if label:\n",
    "        mx, my = (x1+x2)/2, (y1+y2)/2\n",
    "        ax.text(mx+0.1, my, label, fontsize=8, color=color)\n",
    "\n",
    "# Input: Reference Audio\n",
    "draw_box(ax, 0.5, 5.5, 2.5, 1.2, \"Reference Audio\\n(3-10 seconds)\\nfor voice cloning\", '#9C27B0')\n",
    "\n",
    "# NeuCodec Encoder\n",
    "draw_box(ax, 0.5, 3.5, 2.5, 1.5, \"NeuCodec\\nEncoder\\n(CNN+TCN+RVQ)\", '#673AB7')\n",
    "\n",
    "# Input: Text\n",
    "draw_box(ax, 4.5, 5.5, 2.5, 1.2, \"Text Input\\n'Xin chào Việt Nam'\\n(Vietnamese)\", '#1976D2')\n",
    "\n",
    "# G2P + Tokenizer\n",
    "draw_box(ax, 4.5, 3.5, 2.5, 1.5, \"G2P +\\nTokenizer\\n(phonemes → IDs)\", '#0288D1')\n",
    "\n",
    "# LLM\n",
    "draw_box(ax, 4.0, 1.0, 5.5, 2.0, \"VieNeu Language Model\\n(Transformer, ~1B params)\\nAutoregressive next-token prediction\", '#E65100', fontsize=10)\n",
    "\n",
    "# NeuCodec Decoder\n",
    "draw_box(ax, 10.5, 3.5, 2.5, 1.5, \"NeuCodec\\nDecoder\\n(Transposed CNN)\", '#673AB7')\n",
    "\n",
    "# Output\n",
    "draw_box(ax, 10.5, 5.5, 2.5, 1.2, \"Output Audio\\nCloned Voice\\n(Vietnamese TTS)\", '#2E7D32')\n",
    "\n",
    "# Arrows\n",
    "draw_arrow(ax, 1.75, 5.5, 1.75, 5.0, '', '#9C27B0')   # ref audio → codec\n",
    "draw_arrow(ax, 1.75, 3.5, 4.0, 2.5, 'ref speech\\ntokens', '#673AB7')  # codec → LLM\n",
    "draw_arrow(ax, 5.75, 5.5, 5.75, 5.0, '', '#1976D2')   # text → G2P\n",
    "draw_arrow(ax, 5.75, 3.5, 5.75, 3.0, 'text\\ntokens', '#0288D1')  # G2P → LLM\n",
    "draw_arrow(ax, 7.5, 1.0, 11.75, 3.5, 'predicted speech\\ntokens', '#E65100')  # LLM → decoder\n",
    "draw_arrow(ax, 11.75, 5.0, 11.75, 5.5, '', '#2E7D32')  # decoder → output\n",
    "\n",
    "# Training vs Inference labels\n",
    "ax.text(8.0, 0.4, 'Training: Cross-entropy loss on speech token prediction',\n",
    "        ha='center', fontsize=9, color='gray', style='italic')\n",
    "ax.text(8.0, 0.15, 'Inference: Autoregressive generation until SPEECH_GEN_END token',\n",
    "        ha='center', fontsize=9, color='gray', style='italic')\n",
    "\n",
    "# Vocabulary note\n",
    "vocab_box = patches.FancyBboxPatch((3.5, 6.8), 9, 0.9, boxstyle=\"round,pad=0.1\",\n",
    "                                    facecolor='#FFF3E0', edgecolor='orange', linewidth=2)\n",
    "ax.add_patch(vocab_box)\n",
    "ax.text(8.0, 7.25,\n",
    "        \"Unified Vocabulary: [text tokens: 0-32767] + [speech tokens: 32768-65535] + [special tokens]\",\n",
    "        ha='center', va='center', fontsize=9, color='#E65100', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch03_llm_architecture.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Architecture summary:\")\n",
    "print(\"  1. Reference audio → NeuCodec Encoder → reference speech tokens (int IDs)\")\n",
    "print(\"  2. Text → G2P → Tokenizer → text token IDs\")\n",
    "print(\"  3. LLM takes [text tokens + ref tokens] as context\")\n",
    "print(\"  4. LLM predicts speech tokens autoregressively\")\n",
    "print(\"  5. Predicted speech tokens → NeuCodec Decoder → waveform\")\n",
    "print()\n",
    "print(\"Zero-shot cloning: Steps 1 and 3-5. No fine-tuning on the new speaker.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "## 7. Architecture Comparison Table\n",
    "\n",
    "A comprehensive comparison of all TTS architectures, rendered as a styled pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Era\": [\n",
    "        \"Concatenative\\n(1990s–2010s)\",\n",
    "        \"HMM Parametric\\n(2000s–2015)\",\n",
    "        \"Tacotron 2\\n(2018)\",\n",
    "        \"FastSpeech 2\\n(2021)\",\n",
    "        \"VITS\\n(2021)\",\n",
    "        \"VieNeu-TTS\\n(2024)\",\n",
    "    ],\n",
    "    \"Input\": [\"Phonemes\", \"Phonemes\", \"Phonemes\", \"Phonemes\", \"Phonemes\", \"Phonemes + Ref Audio\"],\n",
    "    \"Intermediate\": [\"Audio units\", \"Acoustic features\", \"Mel spectrogram\", \"Mel spectrogram\", \"Latent z\", \"Speech tokens\"],\n",
    "    \"Output\": [\"Waveform\", \"Waveform\", \"Waveform\", \"Waveform\", \"Waveform\", \"Waveform\"],\n",
    "    \"Vocoder\": [\"None (direct)\", \"WORLD/STRAIGHT\", \"WaveNet/HiFiGAN\", \"HiFi-GAN\", \"None (E2E)\", \"NeuCodec decoder\"],\n",
    "    \"Autoregressive\": [\"No\", \"No\", \"Yes (decoder)\", \"No\", \"No\", \"Yes (LLM)\"],\n",
    "    \"Training Data\": [\"5-20h single speaker\", \"5-20h single speaker\", \"20-50h single spkr\", \"20-50h + alignments\", \"20-50h single spkr\", \"100h+ multi-speaker\"],\n",
    "    \"Vietnamese Tones\": [\"Poor\", \"OK\", \"Good\", \"Good\", \"Good\", \"Excellent\"],\n",
    "    \"Phonation Type\": [\"Excellent (real)\", \"Poor\", \"Fair\", \"Fair\", \"Good (GAN)\", \"Good (codec)\"],\n",
    "    \"Multi-speaker\": [\"No\", \"Limited\", \"No\", \"Yes (embed)\", \"Yes (embed)\", \"Yes (zero-shot)\"],\n",
    "    \"Zero-shot Cloning\": [\"No\", \"No\", \"No\", \"No\", \"Limited\", \"Yes\"],\n",
    "    \"Approx RTF (CPU)\": [\"<1x (fast)\", \"~1x\", \"~10x\", \"~3x\", \"~5x\", \"~2-5x\"],\n",
    "    \"Key Failure Mode\": [\n",
    "        \"DB coverage gaps\",\n",
    "        \"Over-smoothing\",\n",
    "        \"Attention errors\",\n",
    "        \"Monotone prosody\",\n",
    "        \"Training instability\",\n",
    "        \"Slow autoregressive\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.set_index(\"Era\")\n",
    "\n",
    "# Display\n",
    "print(\"TTS Architecture Evolution Comparison\")\n",
    "print(\"=\" * 120)\n",
    "print(df.to_string())\n",
    "\n",
    "# Also render as styled HTML (works in Jupyter)\n",
    "try:\n",
    "    styled = (\n",
    "        df.style\n",
    "        .set_caption(\"TTS Architecture Evolution — VieNeu-TTS Context\")\n",
    "        .highlight_max(subset=['Vietnamese Tones'], color='lightgreen')  # removed .text()\n",
    "        .set_properties(**{'text-align': 'center', 'font-size': '10px'})\n",
    "        .set_table_styles([{\n",
    "            'selector': 'caption',\n",
    "            'props': [('font-size', '14px'), ('font-weight', 'bold')]\n",
    "        }])\n",
    "    )\n",
    "    display(styled)\n",
    "except Exception:\n",
    "    pass  # Already printed the table above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9b0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the quality vs latency trade-off across architectures\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "systems = [\n",
    "    {\"name\": \"Concatenative\",  \"rtf\": 0.5,  \"quality\": 3.5, \"viet_tone\": 2.0, \"size\": 100, \"color\": \"#607D8B\"},\n",
    "    {\"name\": \"HMM Parametric\", \"rtf\": 1.0,  \"quality\": 3.2, \"viet_tone\": 3.0, \"size\": 100, \"color\": \"#795548\"},\n",
    "    {\"name\": \"Tacotron 2\",     \"rtf\": 10.0, \"quality\": 4.0, \"viet_tone\": 3.8, \"size\": 200, \"color\": \"#FF9800\"},\n",
    "    {\"name\": \"FastSpeech 2\",   \"rtf\": 3.0,  \"quality\": 3.8, \"viet_tone\": 3.8, \"size\": 200, \"color\": \"#4CAF50\"},\n",
    "    {\"name\": \"VITS\",           \"rtf\": 5.0,  \"quality\": 4.3, \"viet_tone\": 4.0, \"size\": 200, \"color\": \"#2196F3\"},\n",
    "    {\"name\": \"VieNeu-TTS\",     \"rtf\": 3.5,  \"quality\": 4.7, \"viet_tone\": 4.8, \"size\": 400, \"color\": \"#F44336\"},\n",
    "]\n",
    "\n",
    "# Quality vs RTF bubble chart\n",
    "# Bubble size = Vietnamese tone quality\n",
    "for sys in systems:\n",
    "    scatter = ax.scatter(sys[\"rtf\"], sys[\"quality\"],\n",
    "                         s=sys[\"viet_tone\"] * 150,  # bubble size = tone quality\n",
    "                         c=sys[\"color\"],\n",
    "                         alpha=0.8, edgecolors='black', linewidth=1.5,\n",
    "                         zorder=3)\n",
    "    offset_x = 0.2\n",
    "    offset_y = 0.05\n",
    "    if sys['name'] == 'HMM Parametric':\n",
    "        offset_y = -0.12\n",
    "    if sys['name'] == 'Tacotron 2':\n",
    "        offset_x = -1.5\n",
    "        offset_y = 0.08\n",
    "    ax.annotate(sys[\"name\"],\n",
    "                xy=(sys[\"rtf\"], sys[\"quality\"]),\n",
    "                xytext=(sys[\"rtf\"] + offset_x, sys[\"quality\"] + offset_y + 0.08),\n",
    "                fontsize=10, fontweight='bold', color=sys[\"color\"],\n",
    "                ha='left')\n",
    "\n",
    "# Reference lines\n",
    "ax.axvline(1.0, color='gray', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "ax.text(1.05, 2.9, 'RTF=1 (real-time)', fontsize=9, color='gray', rotation=90)\n",
    "ax.axhline(4.0, color='gray', linestyle=':', alpha=0.5, linewidth=1.5)\n",
    "ax.text(0.3, 4.02, 'MOS=4.0 (threshold for natural-sounding)', fontsize=9, color='gray')\n",
    "\n",
    "ax.set_xlabel(\"Real-Time Factor (CPU) — Lower is Faster\", fontsize=12)\n",
    "ax.set_ylabel(\"Estimated MOS (Mean Opinion Score) — Higher is Better\", fontsize=12)\n",
    "ax.set_title(\"TTS Architecture Trade-off: Quality vs. Speed\\n\"\n",
    "             \"Bubble size = Vietnamese tone accuracy (larger = better)\",\n",
    "             fontsize=13, fontweight='bold')\n",
    "ax.set_xlim(-0.5, 12)\n",
    "ax.set_ylim(2.8, 5.0)\n",
    "ax.set_xscale('linear')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Arrow showing desired direction\n",
    "ax.annotate('Desired direction\\n(faster + better quality)', xy=(2.0, 4.9),\n",
    "            xytext=(5, 4.7),\n",
    "            arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
    "            fontsize=9, color='darkgreen')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch03_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"  VieNeu-TTS achieves the best quality AND Vietnamese tone accuracy.\")\n",
    "print(\"  It trades off some speed (autoregressive) for the LLM's language understanding.\")\n",
    "print(\"  For production: use streaming + GPU to achieve real-time performance.\")\n",
    "print()\n",
    "print(\"Next steps:\")\n",
    "print(\"  Chapter 04: NeuCodec — Neural Audio Codec Architecture and Training\")\n",
    "print(\"  Chapter 05: VieNeu Language Model — Transformer Architecture for TTS\")\n",
    "print(\"  Chapter 06: Training VieNeu-TTS from Scratch on Vietnamese Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0c1d2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We traced the evolution of TTS from the 1990s to 2024:\n",
    "\n",
    "| Era | Core Idea | Vietnamese Challenge | Solution |\n",
    "|-----|-----------|--------------------|---------|\n",
    "| **Concatenative** | Record + concatenate speech units | Huge inventory for 6 tones | Expensive corpus collection |\n",
    "| **HMM Parametric** | Statistical model of acoustic features | Over-smoothing blurs tone F0 | MLPG + better feature modeling |\n",
    "| **Tacotron 2** | Seq2seq with attention alignment | Attention fails at tone boundaries | Location-sensitive attention |\n",
    "| **FastSpeech 2** | Non-autoregressive with duration predictor | Pitch predictor must learn 6 F0 contours | Separate pitch/duration/energy predictors |\n",
    "| **VITS** | End-to-end VAE+GAN | Phonation type modeling | GAN discriminator enforces natural phonation |\n",
    "| **VieNeu-TTS** | LLM + neural codec | Code-switching, zero-shot cloning | LLM understands language; codec discretizes speech |\n",
    "\n",
    "**The key architectural insight**: Vietnamese TTS has always been harder than English TTS due to the 6-tone system requiring both F0 contour AND phonation type modeling. The LLM approach naturally solves this because:\n",
    "1. The LLM understands tone marks as linguistic features (from text pretraining)\n",
    "2. The NeuCodec first codebook encodes tonal information in the discrete speech tokens\n",
    "3. In-context learning enables zero-shot speaker adaptation without any architectural changes\n",
    "\n",
    "Continue to **Chapter 04** to learn about the NeuCodec neural audio codec that makes LLM-TTS possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
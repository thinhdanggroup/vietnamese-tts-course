{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ch07-cell-01",
   "metadata": {},
   "source": [
    "# Chapter 7 — LoRA Fine-tuning Theory: Adapting VieNeu-TTS to Your Voice"
   ]
  },
  {
   "id": "colab-setup-ch07",
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Colab auto-setup (safe to run locally too) ───────────────────\nimport sys, os\n\nif \"google.colab\" in sys.modules:\n    if not os.path.exists(\"/content/vietnamese-tts-course\"):\n        !git clone https://github.com/thinhdanggroup/vietnamese-tts-course.git /content/vietnamese-tts-course\n    os.chdir(\"/content/vietnamese-tts-course/chapters\")\n    !pip install -q librosa soundfile matplotlib\n\n    if not os.path.exists(\"/content/VieNeu-TTS\"):\n        !git clone https://github.com/pnnbao97/VieNeu-TTS.git /content/VieNeu-TTS\n        !pip install -q -e /content/VieNeu-TTS\n    sys.path.insert(0, \"/content/VieNeu-TTS\")\n    print(\"Colab setup complete.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch07-cell-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch07-cell-03",
   "metadata": {},
   "source": [
    "## 1. Memory Cost of Fine-tuning — Why We Need LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch07-cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_finetune_memory(num_params_M, dtype_bytes=4, batch_size=2,\n",
    "                              seq_len=2048, hidden_dim=1024, num_layers=24):\n",
    "    weights_GB = num_params_M * 1e6 * dtype_bytes / 1e9\n",
    "    gradients_GB = weights_GB\n",
    "    adam_states_GB = 2 * weights_GB\n",
    "    activation_GB = (batch_size * seq_len * hidden_dim * num_layers * dtype_bytes) / 1e9\n",
    "    total_GB = weights_GB + gradients_GB + adam_states_GB + activation_GB\n",
    "    return {\n",
    "        \"weights\": weights_GB,\n",
    "        \"gradients\": gradients_GB,\n",
    "        \"adam_states\": adam_states_GB,\n",
    "        \"activations\": activation_GB,\n",
    "        \"total\": total_GB\n",
    "    }\n",
    "\n",
    "mem = estimate_finetune_memory(300)\n",
    "print(\"=== Full Fine-tuning Memory Estimate (VieNeu-TTS-0.3B) ===\")\n",
    "for k, v in mem.items():\n",
    "    print(f\"  {k:15s}: {v:.2f} GB\")\n",
    "print(f\"\\nRequired GPU VRAM: >{mem['total']:.0f} GB\")\n",
    "print(f\"RTX 3090 (24 GB): {'OK' if mem['total'] < 24 else 'OOM'}\")\n",
    "print(f\"RTX 3060 (12 GB): {'OK' if mem['total'] < 12 else 'OOM'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch07-cell-05",
   "metadata": {},
   "source": [
    "## 2. LoRA: Low-Rank Approximation — The Math\n",
    "\n",
    "For a weight matrix $W \\in \\mathbb{R}^{d \\times k}$, LoRA represents the update as:\n",
    "$$\\Delta W = BA, \\quad B \\in \\mathbb{R}^{d \\times r},\\ A \\in \\mathbb{R}^{r \\times k}, \\quad r \\ll \\min(d, k)$$\n",
    "\n",
    "The full forward pass becomes:\n",
    "$$h = (W + \\frac{\\alpha}{r} BA)x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch07-cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "d, k = 64, 64\n",
    "W = np.random.randn(d, k) * 0.1\n",
    "W_finetuned = W + np.outer(np.random.randn(d), np.random.randn(k)) * 0.05\n",
    "delta_W = W_finetuned - W\n",
    "U, sigma, Vt = np.linalg.svd(delta_W)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(sigma, 'b-o', markersize=4)\n",
    "axes[0].axvline(x=16, color='r', linestyle='--', label='r=16 cutoff')\n",
    "axes[0].set_xlabel(\"Singular value index\")\n",
    "axes[0].set_ylabel(\"Singular value magnitude\")\n",
    "axes[0].set_title(\"Singular Values of delta_W\\n(Most energy in first few components)\")\n",
    "axes[0].legend()\n",
    "\n",
    "r_values = [1, 2, 4, 8, 16, 32, 64]\n",
    "errors = []\n",
    "for r in r_values:\n",
    "    approx = U[:, :r] @ np.diag(sigma[:r]) @ Vt[:r, :]\n",
    "    err = np.linalg.norm(delta_W - approx, 'fro') / np.linalg.norm(delta_W, 'fro')\n",
    "    errors.append(err)\n",
    "    print(f\"r={r:3d}: params={d*r + r*k:6d} ({(d*r+r*k)/(d*k)*100:.1f}%), error={err:.4f}\")\n",
    "\n",
    "axes[1].semilogx(r_values, errors, 'g-o')\n",
    "axes[1].set_xlabel(\"LoRA rank r\")\n",
    "axes[1].set_ylabel(\"Relative reconstruction error\")\n",
    "axes[1].set_title(\"LoRA Rank vs Approximation Quality\")\n",
    "axes[1].axhline(y=0.01, color='r', linestyle='--', label='1% error')\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch07-cell-07",
   "metadata": {},
   "source": [
    "## 3. Parameter Count: LoRA vs Full Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch07-cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_params(d, k, r): return d * r + r * k\n",
    "def full_params(d, k): return d * k\n",
    "\n",
    "layers = {\n",
    "    \"q_proj (1024x1024)\": (1024, 1024),\n",
    "    \"k_proj (1024x1024)\": (1024, 1024),\n",
    "    \"v_proj (1024x1024)\": (1024, 1024),\n",
    "    \"o_proj (1024x1024)\": (1024, 1024),\n",
    "    \"gate_proj (1024x4096)\": (1024, 4096),\n",
    "    \"up_proj (1024x4096)\": (1024, 4096),\n",
    "    \"down_proj (4096x1024)\": (4096, 1024),\n",
    "}\n",
    "r = 16\n",
    "num_layers = 24\n",
    "rows = []\n",
    "for name, (d, k) in layers.items():\n",
    "    full = full_params(d, k)\n",
    "    lora = lora_params(d, k, r)\n",
    "    rows.append({\"Layer\": name, \"Full params\": full,\n",
    "                 \"LoRA params (r=16)\": lora, \"Savings\": f\"{full//lora}x\"})\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_string())\n",
    "total_lora = sum(lora_params(d,k,r) * num_layers for d,k in layers.values())\n",
    "total_full = sum(full_params(d,k) * num_layers for d,k in layers.values())\n",
    "print(f\"\\nTotal LoRA trainable: {total_lora:,}\")\n",
    "print(f\"Total full finetune:  {total_full:,}\")\n",
    "print(f\"LoRA ratio: {total_lora/total_full*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch07-cell-09",
   "metadata": {},
   "source": [
    "## 4. LoRA Memory Savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch07-cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_memory_GB(trainable_params_M):\n",
    "    base = 300 * 1e6 * 2 / 1e9  # frozen model fp16\n",
    "    lora = trainable_params_M * 1e6 * 4 / 1e9\n",
    "    overhead = lora + lora + 2 * lora  # weights + grad + adam\n",
    "    return base + overhead, base, overhead\n",
    "\n",
    "total, base, overhead = lora_memory_GB(4)\n",
    "print(\"=== LoRA Memory Estimate ===\")\n",
    "print(f\"  Frozen base (fp16): {base:.2f} GB\")\n",
    "print(f\"  LoRA overhead:      {overhead:.2f} GB\")\n",
    "print(f\"  Total:              {total:.2f} GB\")\n",
    "print(f\"\\n  RTX 3060 (12 GB): {'OK' if total < 12 else 'OOM'}\")\n",
    "print(f\"  RTX 3090 (24 GB): OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch07-cell-11",
   "metadata": {},
   "source": [
    "## 5. Rank Sensitivity — Choosing r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch07-cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_vals = [4, 8, 16, 32, 64]\n",
    "d, k = 1024, 1024\n",
    "num_layers = 24\n",
    "modules_per_layer = 7\n",
    "\n",
    "params = [lora_params(d, k, r) * num_layers * modules_per_layer for r in r_vals]\n",
    "quality_proxy = [1 - 0.3 * np.exp(-r / 10) for r in r_vals]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.bar([str(r) for r in r_vals], [p/1e6 for p in params], alpha=0.6, color='steelblue', label='Trainable params (M)')\n",
    "ax2.plot([str(r) for r in r_vals], quality_proxy, 'ro-', label='Quality proxy')\n",
    "ax1.set_xlabel(\"LoRA rank r\")\n",
    "ax1.set_ylabel(\"Trainable parameters (M)\", color='steelblue')\n",
    "ax2.set_ylabel(\"Relative quality\", color='red')\n",
    "ax1.set_title(\"LoRA Rank Trade-off: Parameters vs Quality\")\n",
    "ax1.axvline(x=2, color='green', linestyle='--', alpha=0.7, label='Recommended r=16')\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for r, p, q in zip(r_vals, params, quality_proxy):\n",
    "    print(f\"r={r:3d}: {p/1e6:.1f}M params, quality={q:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch07-cell-13",
   "metadata": {},
   "source": [
    "## 6. LoRA Hyperparameter Cheat Sheet for VieNeu-TTS\n",
    "\n",
    "| Parameter | Value | Notes |\n",
    "|-----------|-------|-------|\n",
    "| `r` | 16 | Rank — good balance for single voice |\n",
    "| `lora_alpha` | 32 | Always 2× rank |\n",
    "| `lora_dropout` | 0.05 | Light regularization |\n",
    "| `target_modules` | q/k/v/o + MLP | All attention + feed-forward |\n",
    "| `learning_rate` | 2e-4 | Higher than full fine-tune |\n",
    "| `max_steps` | 5000 | For ~1000 clips |\n",
    "| `batch_size` | 2 | Reduce to 1 if OOM |\n",
    "| `warmup_ratio` | 0.05 | First 5% of steps |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ch10-cell-01",
   "metadata": {},
   "source": ["# Chapter 10 — Deployment & Optimization"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch10-cell-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch10-cell-03",
   "metadata": {},
   "source": [
    "## 1. Quantization: How Q4 Compresses the Model\n",
    "\n",
    "Linear quantization maps float weights to integers:\n",
    "$$q = \\text{round}\\left(\\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} \\cdot (2^b - 1)\\right), \\quad x_{\\text{approx}} = q \\cdot s + x_{\\min}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch10-cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(weights, bits):\n",
    "    w_min, w_max = weights.min(), weights.max()\n",
    "    n_levels = 2**bits\n",
    "    scale = (w_max - w_min) / (n_levels - 1)\n",
    "    q = np.round((weights - w_min) / scale).astype(np.int32)\n",
    "    deq = q * scale + w_min\n",
    "    return deq, np.abs(weights - deq)\n",
    "\n",
    "np.random.seed(42)\n",
    "weights = np.random.randn(1000) * 0.05\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, bits in zip(axes, [8, 4, 2]):\n",
    "    deq, err = quantize(weights, bits)\n",
    "    ax.scatter(weights, deq, alpha=0.15, s=2)\n",
    "    ax.plot([-0.2, 0.2], [-0.2, 0.2], 'r--', lw=1, label=\"Perfect\")\n",
    "    ax.set_title(f\"Q{bits}: max error={err.max():.5f}\")\n",
    "    ax.set_xlabel(\"Original\"); ax.set_ylabel(\"Dequantized\")\n",
    "    ax.legend(fontsize=8)\n",
    "plt.suptitle(\"Weight Quantization — Q4 is nearly lossless for TTS\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch10-cell-05",
   "metadata": {},
   "source": ["## 2. Memory Footprint by Format"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch10-cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for model_name, params_M in [(\"VieNeu-TTS-0.3B\", 300), (\"VieNeu-TTS-0.5B\", 500)]:\n",
    "    for fmt, bits in [(\"fp32\", 32), (\"fp16\", 16), (\"Q8\", 8), (\"Q4\", 4)]:\n",
    "        gb = params_M * 1e6 * (bits/8) / 1e9\n",
    "        total = gb + 0.05  # codec\n",
    "        rows.append({\n",
    "            \"Model\": model_name, \"Format\": fmt,\n",
    "            \"Backbone (GB)\": f\"{gb:.2f}\",\n",
    "            \"Total w/ codec\": f\"{total:.2f}\",\n",
    "            \"Fits 8GB RAM\": \"Yes\" if total < 8 else \"No\",\n",
    "            \"Fits 16GB RAM\": \"Yes\" if total < 16 else \"No\",\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch10-cell-07",
   "metadata": {},
   "source": ["## 3. RTF Benchmark"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch10-cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from vieneu import Vieneu\n",
    "\n",
    "test_texts = [\n",
    "    \"Xin ch\\u00e0o.\",\n",
    "    \"H\\u00f4m nay th\\u1eddi ti\\u1ebft r\\u1ea5t \\u0111\\u1eb9p, t\\u00f4i mu\\u1ed1n \\u0111i d\\u1ea1o.\",\n",
    "    \"Tr\\u00ed tu\\u1ec7 nh\\u00e2n t\\u1ea1o \\u0111ang ph\\u00e1t tri\\u1ec3n nhanh ch\\u00f3ng v\\u00e0 thay \\u0111\\u1ed5i nhi\\u1ec1u ng\\u00e0nh c\\u00f4ng nghi\\u1ec7p.\",\n",
    "]\n",
    "\n",
    "tts = Vieneu()\n",
    "results = []\n",
    "for text in test_texts:\n",
    "    times = []\n",
    "    for _ in range(2):\n",
    "        t0 = time.time()\n",
    "        audio = tts.infer(text)\n",
    "        times.append(time.time() - t0)\n",
    "    avg = np.mean(times)\n",
    "    dur = len(audio) / 24000\n",
    "    rtf = avg / dur\n",
    "    results.append({\n",
    "        \"Text\": text[:45] + (\"...\" if len(text)>45 else \"\"),\n",
    "        \"Audio (s)\": f\"{dur:.2f}\",\n",
    "        \"Infer (s)\": f\"{avg:.2f}\",\n",
    "        \"RTF\": f\"{rtf:.2f}x\",\n",
    "        \"Real-time\": \"Yes\" if rtf < 1.0 else \"No\"\n",
    "    })\n",
    "tts.close()\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nRTF < 1.0 = faster than real-time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch10-cell-09",
   "metadata": {},
   "source": ["## 4. Streaming Inference — First-Chunk Latency"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch10-cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vieneu import Vieneu\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "tts = Vieneu()\n",
    "text = \"Tr\\u00ed tu\\u1ec7 nh\\u00e2n t\\u1ea1o \\u0111ang thay \\u0111\\u1ed5i c\\u00e1ch ch\\u00fang ta giao ti\\u1ebfp h\\u00e0ng ng\\u00e0y.\"\n",
    "voice = tts.get_preset_voice(\"Binh\")\n",
    "\n",
    "chunks = []\n",
    "chunk_times = []\n",
    "t_start = time.time()\n",
    "\n",
    "for chunk in tts.infer_stream(text, voice=voice):\n",
    "    t_chunk = time.time() - t_start\n",
    "    chunk_times.append(t_chunk)\n",
    "    chunks.append(chunk)\n",
    "    print(f\"Chunk {len(chunks):2d}: {len(chunk)/24000:.2f}s audio at t={t_chunk:.2f}s\")\n",
    "\n",
    "full_audio = np.concatenate(chunks)\n",
    "print(f\"\\nFirst-chunk latency: {chunk_times[0]:.2f}s\")\n",
    "print(f\"Total audio: {len(full_audio)/24000:.2f}s\")\n",
    "display(Audio(full_audio, rate=24000))\n",
    "tts.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch10-cell-11",
   "metadata": {},
   "source": ["## 5. Packaging voices.json"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ch10-cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, librosa\n",
    "from neucodec import DistillNeuCodec\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "codec = DistillNeuCodec.from_pretrained(\"neuphonic/distill-neucodec\").to(device).eval()\n",
    "\n",
    "ref_path = \"../examples/audio_ref/example.wav\"\n",
    "ref_text = open(\"../examples/audio_ref/example.txt\").read().strip()\n",
    "\n",
    "wav, sr = librosa.load(ref_path, sr=16000, mono=True)\n",
    "wav_t = torch.from_numpy(wav).float().unsqueeze(0).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    codes = codec.encode_code(wav_t).squeeze().cpu().numpy().flatten().tolist()\n",
    "\n",
    "voices_json = {\n",
    "    \"default_voice\": \"example_voice\",\n",
    "    \"presets\": {\n",
    "        \"example_voice\": {\n",
    "            \"codes\": [int(c) for c in codes],\n",
    "            \"text\": ref_text,\n",
    "            \"description\": \"Example Vietnamese voice\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"voices.json structure:\")\n",
    "print(f\"  default_voice: {voices_json['default_voice']}\")\n",
    "print(f\"  text: {ref_text[:60]}...\")\n",
    "print(f\"  codes[:10]: {voices_json['presets']['example_voice']['codes'][:10]}\")\n",
    "print(f\"  total tokens: {len(codes)}\")\n",
    "print(f\"\\nWith this file, users can run:\")\n",
    "print(\"  tts = Vieneu(backbone_repo='your-username/your-model')\")\n",
    "print(\"  audio = tts.infer('Xin chao!')  # no ref_audio needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch10-cell-13",
   "metadata": {},
   "source": [
    "## 6. Production Deployment Checklist\n",
    "\n",
    "| Category | Check |\n",
    "|----------|-------|\n",
    "| **Model** | Use GGUF Q4 for CPU deployment |\n",
    "| **Model** | Merge LoRA before distributing |\n",
    "| **Model** | Bundle voices.json on HuggingFace |\n",
    "| **Quality** | Test all 6 Vietnamese tones |\n",
    "| **Quality** | Test code-switching (Vietnamese + English) |\n",
    "| **Quality** | UTMOS > 3.8 on test sentences |\n",
    "| **Quality** | Human MOS on 30+ sentences |\n",
    "| **Performance** | RTF < 1.0 on target hardware |\n",
    "| **Serving** | Use streaming for conversational apps |\n",
    "| **Serving** | Set max_chars=256 for natural chunking |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ch10-cell-14",
   "metadata": {},
   "source": [
    "## Course Complete!\n",
    "\n",
    "You have now covered the full end-to-end VieNeu-TTS pipeline:\n",
    "\n",
    "```\n",
    "Audio Fundamentals → Text Processing → TTS Architectures → Neural Codecs\n",
    "                                                                    ↓\n",
    "                                                           LLM-Based TTS\n",
    "                                                           ↙           ↘\n",
    "                                                  Voice Cloning    LoRA Theory\n",
    "                                                                        ↓\n",
    "                                                               Data Preparation\n",
    "                                                                        ↓\n",
    "                                                          Training & Evaluation\n",
    "                                                                        ↓\n",
    "                                                                  Deployment\n",
    "```\n",
    "\n",
    "**Next steps:**\n",
    "1. Collect your Vietnamese audio dataset (Chapter 8)\n",
    "2. Run the fine-tuning pipeline (Chapter 9)\n",
    "3. Deploy your custom model (Chapter 10)\n",
    "4. Contribute to the VieNeu-TTS community!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
